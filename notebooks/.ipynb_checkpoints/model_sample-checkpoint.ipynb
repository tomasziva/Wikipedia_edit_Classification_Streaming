{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Tomas-PC:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD SOME RELEVANT LIBRARIES -- OTHERS WILL BE LOADED WHEN NEEDED\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOP THROUGH ALL FILES\n",
    "\n",
    "import os\n",
    "rootdir = 'C:/Pr/AA_Big_Data/Assignment_3/spark/save_30'\n",
    "\n",
    "files_list = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for name in files:\n",
    "        if \"part\" in name.lower() and not \".crc\" in name.lower():\n",
    "            files_list.append(os.path.join(subdir,name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             comment| label|           name_user|            text_new|            text_old|          title_page|            url_page|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                prod|  safe|            Reywas92|{{Proposed deleti...|{{geoGroupTemplat...|List of cemeterie...|//en.wikipedia.or...|\n",
      "|→‎New Zealand:upd...|  safe|            Flags220|{{short descripti...|{{short descripti...|Sequoiadendron gi...|//en.wikipedia.or...|\n",
      "|→‎Landmarks:added...|  safe|         Engrksaim29|{{Use British Eng...|{{Use British Eng...|Barnala, Azad Kas...|//en.wikipedia.or...|\n",
      "|                prod|  safe|            Reywas92|{{Proposed deleti...|{{geoGroupTemplat...|List of cemeterie...|//en.wikipedia.or...|\n",
      "|small changes mad...|  safe|   Priti Rao Krishna|[[File:Hyder Kazm...|[[File:Hyder Kazm...|         Hyder Kazmi|//en.wikipedia.or...|\n",
      "|Update place of b...|  safe|          Jdyates751|{{for|the footbal...|{{for|the footbal...|   Alejandro R. Ruiz|//en.wikipedia.or...|\n",
      "|        →‎Population|  safe|              Jaa101|{{Infobox French ...|{{Infobox French ...|  Chaumont-sur-Loire|//en.wikipedia.or...|\n",
      "|           →‎Summary|  safe|         HugoAcosta9|{{Infobox footbal...|{{Infobox footbal...|1966–67 Inter Mil...|//en.wikipedia.or...|\n",
      "|bypass redirect, ...|  safe|       RobertskySemi|{{pp-protected|sm...|{{pp-protected|sm...|COVID-19 pandemic...|//en.wikipedia.or...|\n",
      "|                    |  safe|           Hhfjbaker|{{Infobox Militar...|{{Infobox Militar...|  61st Ohio Infantry|//en.wikipedia.or...|\n",
      "|     (Added content)|unsafe|      24.220.101.210|{{short descripti...|{{short descripti...|          Waco siege|//en.wikipedia.or...|\n",
      "|bypass redirect, ...|  safe|       RobertskySemi|{{Use dmy dates|d...|{{Use dmy dates|d...|COVID-19 pandemic...|//en.wikipedia.or...|\n",
      "|                    |unsafe|2a02:c7f:4829:180...|'''[[Salman Khan]...|'''[[Salman Khan]...|Salman Khan (disa...|//en.wikipedia.or...|\n",
      "|      added sentence|  safe|              Xicanx|{{Short descripti...|{{Short descripti...|Communist propaganda|//en.wikipedia.or...|\n",
      "|                    |unsafe|2601:681:4202:983...|{{short descripti...|{{short descripti...|             Peafowl|//en.wikipedia.or...|\n",
      "|                    |  safe|            Spadroon|{{About|the Briti...|{{About|the Briti...|Diamond Head (Eng...|//en.wikipedia.or...|\n",
      "|defaults to the c...|  safe|               Uzume|{{Infobox settlem...|{{Infobox settlem...|           Mabalacat|//en.wikipedia.or...|\n",
      "|Update place of b...|  safe|          Jdyates751|{{Infobox militar...|{{Infobox militar...|     James H. Fields|//en.wikipedia.or...|\n",
      "|                prod|  safe|            Reywas92|{{Proposed deleti...|{{geoGroup}}\n",
      "{{Ex...|List of cemeterie...|//en.wikipedia.or...|\n",
      "|            clean up|  safe|             Joeykai|'''Sepsivac''' is...|\n",
      "\n",
      "'''Sepsivac''' ...|            Sepsivac|//en.wikipedia.or...|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PUT ALL FILES TO ONE JSON DATAFRAME\n",
    "\n",
    "df1 = spark.read.json(sc.textFile(','.join(files_list)))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- comment: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- name_user: string (nullable = true)\n",
      " |-- text_new: string (nullable = true)\n",
      " |-- text_old: string (nullable = true)\n",
      " |-- title_page: string (nullable = true)\n",
      " |-- url_page: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SHOW SCHEMA OF THE DATASET\n",
    "\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| label|count|\n",
      "+------+-----+\n",
      "|  safe|   61|\n",
      "|vandal|   11|\n",
      "|unsafe|    7|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DISTRIBUTION OF LABELS IN THE DATASET\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df1.groupBy(\"label\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             comment| label|           name_user|            text_new|            text_old|          title_page|            url_page|          difference|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                prod|  safe|            Reywas92|{{Proposed deleti...|{{geoGroupTemplat...|List of cemeterie...|//en.wikipedia.or...|{{Proposed deleti...|\n",
      "|→‎New Zealand:upd...|  safe|            Flags220|{{short descripti...|{{short descripti...|Sequoiadendron gi...|//en.wikipedia.or...|                2530|\n",
      "|→‎Landmarks:added...|  safe|         Engrksaim29|{{Use British Eng...|{{Use British Eng...|Barnala, Azad Kas...|//en.wikipedia.or...|in bitown centre ...|\n",
      "|                prod|  safe|            Reywas92|{{Proposed deleti...|{{geoGroupTemplat...|List of cemeterie...|//en.wikipedia.or...|{{Proposed deleti...|\n",
      "|small changes mad...|  safe|   Priti Rao Krishna|[[File:Hyder Kazm...|[[File:Hyder Kazm...|         Hyder Kazmi|//en.wikipedia.or...|played lead in TV...|\n",
      "|Update place of b...|  safe|          Jdyates751|{{for|the footbal...|{{for|the footbal...|   Alejandro R. Ruiz|//en.wikipedia.or...|                  , |\n",
      "|        →‎Population|  safe|              Jaa101|{{Infobox French ...|{{Infobox French ...|  Chaumont-sur-Loire|//en.wikipedia.or...|                9898|\n",
      "|           →‎Summary|  safe|         HugoAcosta9|{{Infobox footbal...|{{Infobox footbal...|1966–67 Inter Mil...|//en.wikipedia.or...|(included 3 of th...|\n",
      "|bypass redirect, ...|  safe|       RobertskySemi|{{pp-protected|sm...|{{pp-protected|sm...|COVID-19 pandemic...|//en.wikipedia.or...| name=\"bag.admin....|\n",
      "|                    |  safe|           Hhfjbaker|{{Infobox Militar...|{{Infobox Militar...|  61st Ohio Infantry|//en.wikipedia.or...|]]\n",
      "* [[Second Bat...|\n",
      "|     (Added content)|unsafe|      24.220.101.210|{{short descripti...|{{short descripti...|          Waco siege|//en.wikipedia.or...|The Davidians wer...|\n",
      "|bypass redirect, ...|  safe|       RobertskySemi|{{Use dmy dates|d...|{{Use dmy dates|d...|COVID-19 pandemic...|//en.wikipedia.or...|COVID-192020 coro...|\n",
      "|                    |unsafe|2a02:c7f:4829:180...|'''[[Salman Khan]...|'''[[Salman Khan]...|Salman Khan (disa...|//en.wikipedia.or...|                6523|\n",
      "|      added sentence|  safe|              Xicanx|{{Short descripti...|{{Short descripti...|Communist propaganda|//en.wikipedia.or...|]]. Communist pro...|\n",
      "|                    |unsafe|2601:681:4202:983...|{{short descripti...|{{short descripti...|             Peafowl|//en.wikipedia.or...|         eafwlopcock|\n",
      "|                    |  safe|            Spadroon|{{About|the Briti...|{{About|the Briti...|Diamond Head (Eng...|//en.wikipedia.or...|                   '|\n",
      "|defaults to the c...|  safe|               Uzume|{{Infobox settlem...|{{Infobox settlem...|           Mabalacat|//en.wikipedia.or...||id={{#invoke:Wik...|\n",
      "|Update place of b...|  safe|          Jdyates751|{{Infobox militar...|{{Infobox militar...|     James H. Fields|//en.wikipedia.or...|                   ,|\n",
      "|                prod|  safe|            Reywas92|{{Proposed deleti...|{{geoGroup}}\n",
      "{{Ex...|List of cemeterie...|//en.wikipedia.or...|{{Proposed deleti...|\n",
      "|            clean up|  safe|             Joeykai|'''Sepsivac''' is...|\n",
      "\n",
      "'''Sepsivac''' ...|            Sepsivac|//en.wikipedia.or...|\n",
      "\n",
      "\n",
      "[[Category:Sep...|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DIFFERENCE BETWEEN OLD AND NEW TEXT\n",
    "\n",
    "import difflib\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def make_diff(old, new):\n",
    "    diff = difflib.ndiff(old, new)\n",
    "    delta = ''.join(x[2:] for x in diff if x.startswith('- ') or x.startswith('+'))\n",
    "    return delta\n",
    "\n",
    "#convert to a UDF Function and get difference between columns\n",
    "udfmake_diff = F.udf(make_diff, StringType())\n",
    "df_difference1 = df1.withColumn(\"difference\", lit(udfmake_diff(\"text_old\", \"text_new\")))\n",
    "df_difference1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE COLUMN NAME FROM 'LABEL' TO 'LABEL_STRING' -- LATER WE WILL CONVERT LABEL_STRING TO NUMERICAL VALUES AND GIVE THE COLUMN NAME 'LABEL'\n",
    "\n",
    "df_wd1 = df_difference1.withColumnRenamed('label', 'label_string')\n",
    "#df_wd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 59\n",
      "Test Dataset Count: 20\n",
      "Distribution of labels in train set:\n",
      "+------------+-----+\n",
      "|label_string|count|\n",
      "+------------+-----+\n",
      "|        safe|   46|\n",
      "|      vandal|    8|\n",
      "|      unsafe|    5|\n",
      "+------------+-----+\n",
      "\n",
      "Distribution of labels in test set:\n",
      "+------------+-----+\n",
      "|label_string|count|\n",
      "+------------+-----+\n",
      "|        safe|   15|\n",
      "|      vandal|    3|\n",
      "|      unsafe|    2|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EITHER RUN THIS CELL OR THE NEXT CELL (BUT NOT BOTH)\n",
    "# THIS IS RUN ONLY TO TEST MODEL ACCURACY AND TUNE THE MODEL -- SKIP TO TRAIN MODEL ON THE WHOLE DATASET\n",
    "\n",
    "# split dataset to train and test sets\n",
    "\n",
    "(trainingData1, testData1) = df_wd1.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData1.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData1.count()))\n",
    "\n",
    "\n",
    "print(\"Distribution of labels in train set:\")\n",
    "trainingData1.groupBy(\"label_string\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()\n",
    "\n",
    "print(\"Distribution of labels in test set:\")\n",
    "testData1.groupBy(\"label_string\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZING 'DIFFERENCE' COLUMN AND REMOVING CERTAIN STOP WORDS\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "\n",
    "# tokenize 'difference' column\n",
    "regexTokenizer1 = RegexTokenizer(inputCol=\"difference\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "regexTokenizer_comment1 = RegexTokenizer(inputCol=\"comment\", outputCol=\"words_comment\", pattern=\"\\\\W\")\n",
    "\n",
    "# remove stop words\n",
    "stop_words1 = [\"http\",\"https\", \"a\", \"an\", \"the\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"and\", \"any\", \"are\", \"aren't\", \"as\", \"at\", \"be\", \"because\",\n",
    "              \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can't\", \"cannot\", \"could\", \"couldn't\", \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \n",
    "              \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\",\n",
    "              \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\",\n",
    "              \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"mustn't\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \"once\", \"only\",\n",
    "              \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"shan't\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"shouldn't\",\n",
    "              \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\",\n",
    "              \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\",\n",
    "              \"were\", \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"won't\",\n",
    "              \"would\", \"wouldn't\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\"] \n",
    "stopwordsRemover1 = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(stop_words1)\n",
    "stopwordsRemover_comment1 = StopWordsRemover(inputCol=\"words_comment\", outputCol=\"filtered_comment\").setStopWords(stop_words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT 'LABEL_STRING' TO NUMERIC COLUMN 'LABEL'\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# index categorical variable\n",
    "label_stringIdx1 = StringIndexer(inputCol = \"label_string\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF AND PIPELINE -- WITHOUT 'COMMENT'\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# do TF-IDF embeding\n",
    "hashingTF1 = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf1 = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# define the pipeline\n",
    "pipeline1 = Pipeline(stages=[regexTokenizer1, stopwordsRemover1, hashingTF1, idf1, label_stringIdx1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT PIPELINE ON TRAIN DATASET\n",
    "pipelineFit1 = pipeline1.fit(trainingData1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE PIPELINE ON TRAIN DATASET\n",
    "\n",
    "dataset_tr1 = pipelineFit1.transform(trainingData1)\n",
    "\n",
    "# without 'comment':\n",
    "#dataset_tr.select(\"difference\",\"words\",\"filtered\",\"rawFeatures\",\"features\", \"label\").show()\n",
    "\n",
    "# with 'comment':\n",
    "#dataset_tr.select(\"difference\",\"words\",\"filtered\",\"rawFeatures\",\"features_diff\", \"label\").show()\n",
    "#dataset_tr.select(\"comment\",\"words_comment\",\"filtered_comment\",\"rawFeatures_comment\",\"features_comment\", \"label\").show()\n",
    "#dataset_tr.select(\"features_diff\", \"features_comment\", \"features\", \"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE PIPELINE ON TEST DATASET (ONLY DO IF THE SPLIT BETWEEN TRAIN AND TEST SETS WAS PERFORMED)\n",
    "\n",
    "dataset_test1 = pipelineFit1.transform(testData1)\n",
    "\n",
    "# without 'comment':\n",
    "#dataset_test.select(\"difference\",\"words\",\"filtered\",\"rawFeatures\",\"features\", \"label\").show()\n",
    "\n",
    "# with 'comment':\n",
    "#dataset_test.select(\"difference\",\"words\",\"filtered\",\"rawFeatures\",\"features_diff\", \"label\").show()\n",
    "#dataset_test.select(\"comment\",\"words_comment\",\"filtered_comment\",\"rawFeatures_comment\",\"features_comment\", \"label\").show()\n",
    "#dataset_test.select(\"features_diff\", \"features_comment\", \"features\", \"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE WILL USE LOGISTIC REGRESSION FOR PREDICTING\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+--------------------+-----+----------+\n",
      "|             comment|          difference|label_string|         probability|label|prediction|\n",
      "+--------------------+--------------------+------------+--------------------+-----+----------+\n",
      "|                prod|{{Proposed deleti...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|→‎New Zealand:upd...|                2530|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|→‎Landmarks:added...|in bitown centre ...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|small changes mad...|played lead in TV...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|Update place of b...|                  , |        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|bypass redirect, ...| name=\"bag.admin....|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|                    |]]\n",
      "* [[Second Bat...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|     (Added content)|The Davidians wer...|      unsafe|[0.78149239173343...|  2.0|       0.0|\n",
      "|bypass redirect, ...|COVID-192020 coro...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|                    |                6523|      unsafe|[0.78149239173343...|  2.0|       0.0|\n",
      "|      added sentence|]]. Communist pro...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|                    |                   '|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|Update place of b...|                   ,|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|                prod|{{Proposed deleti...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|                prod|{{Proposed deleti...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|                prod|{{Proposed deleti...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|  →‎Reception:punct.|’'’'’'“\"\"”\"’'“\"\"”...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|defaults to the c...||id={{#invoke:Wik...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|           →‎Matches|        TBDAJ Styles|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|→‎April 2020:bypa...|COVID-192020 coro...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "+--------------------+--------------------+------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+------------+--------------------+-----+----------+\n",
      "|             comment|          difference|label_string|         probability|label|prediction|\n",
      "+--------------------+--------------------+------------+--------------------+-----+----------+\n",
      "|                prod|{{Proposed deleti...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|        →‎Population|                9898|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|           →‎Summary|(included 3 of th...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|                    |         eafwlopcock|      unsafe|[0.78149239173343...|  2.0|       0.0|\n",
      "|defaults to the c...||id={{#invoke:Wik...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|            clean up|\n",
      "\n",
      "\n",
      "[[Category:Sep...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|                prod|{{Proposed deleti...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|→‎Biography:linke...|                [[]]|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|→‎Early years and...|]], where she att...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|→‎Environment:cle...|                  e |        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|defaults to the c...||id={{#invoke:Wik...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|          (→‎Arabic)|uuggest[[File:Sev...|      unsafe|[0.76716204191379...|  2.0|       0.0|\n",
      "|Clean upduplicate...|     author=CEHURD ||        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|link was in there...|[[List of Nationa...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|inclusion of Hard...|\n",
      "==*October 1 - E...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|bypass redirect, ...|COVID-192020 coro...|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|                    |         oh hello :v|      vandal|[0.78149239173343...|  1.0|       0.0|\n",
      "|          fewer caps|                SsDd|        safe|[0.78149239173343...|  0.0|       0.0|\n",
      "|                    |aFollow me o tikt...|      vandal|[0.78149239173343...|  1.0|       0.0|\n",
      "|                    |43thfoueefsecurnd...|      vandal|[0.78149239173343...|  1.0|       0.0|\n",
      "+--------------------+--------------------+------------+--------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION WITHOUT WEIGHTS\n",
    "\n",
    "# fit logistic regression\n",
    "lr1 = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel1 = lr1.fit(dataset_tr1)\n",
    "\n",
    "\n",
    "# predict train and test sets\n",
    "predict_train1 = lrModel1.transform(dataset_tr1)\n",
    "predict_test1 = lrModel1.transform(dataset_test1)\n",
    "\n",
    "#comment if not used\n",
    "predict_train1.select(\"comment\", \"difference\",\"label_string\",\"probability\",\"label\",\"prediction\").show()\n",
    "predict_test1.select(\"comment\", \"difference\",\"label_string\",\"probability\",\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
