{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(1000)\n",
    "tf = hashingTF.transform(df_withdiff)\n",
    "\n",
    "# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "# First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "# spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "# which occur in less than a minimum number of documents.\n",
    "# In such cases, the IDF for these terms is set to 0.\n",
    "# This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "tfidfIgnore = idfIgnore.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used alias to avoid confusion with the mllib library\n",
    "from pyspark.ml.feature import HashingTF as MLHashingTF\n",
    "from pyspark.ml.feature import IDF as MLIDF\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "df_withdiff.printSchema()\n",
    "\n",
    "doc = (df_withdiff\n",
    "  .rdd\n",
    "  .map(lambda x : (x.title_page,x.diff.split(\" \")))\n",
    "  .toDF()\n",
    "  .withColumnRenamed(\"_1\",\"title_page\")\n",
    "  .withColumnRenamed(\"_2\",\"diff\"))\n",
    "\n",
    "htf = MLHashingTF(inputCol=\"diff\", outputCol=\"tf\")\n",
    "tf = htf.transform(doc)\n",
    "tf.show(truncate=False)\n",
    "\n",
    "idf = MLIDF(inputCol=\"tf\", outputCol=\"idf\")\n",
    "tfidf = idf.fit(tf).transform(tf)\n",
    "tfidf.show(truncate=False)\n",
    "\n",
    "res = tfidf.rdd.map(lambda x : (x.title_page,x.tf,x.idf,(None if x.idf is None else x.idf.values.sum())))\n",
    "\n",
    "for r in res.take(10):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData=tfidf.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"diff\",\"label_string\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
