{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Tomas-PC:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD SOME RELEVANT LIBRARIES -- OTHERS WILL BE LOADED WHEN NEEDED\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOP THROUGH ALL FILES\n",
    "\n",
    "import os\n",
    "rootdir = 'C:/Pr/AA_Big_Data/Assignment_3/spark/save_2k'\n",
    "\n",
    "files_list = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for name in files:\n",
    "        if \"part\" in name.lower() and not \".crc\" in name.lower():\n",
    "            files_list.append(os.path.join(subdir,name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             comment| label|        name_user|            text_new|            text_old|          title_page|            url_page|\n",
      "+--------------------+------+-----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|→‎Preseason All-S...|  safe|    Debartolo2917|{{short descripti...|{{short descripti...|2019 Kentucky Wil...|//en.wikipedia.or...|\n",
      "|→‎Churches:Fixing...|  safe|           BD2412|{{short descripti...|{{short descripti...|List of churches ...|//en.wikipedia.or...|\n",
      "|→‎Churches holdin...|  safe|           BD2412|[[File:StPaulsCat...|[[File:StPaulsCat...|List of churches ...|//en.wikipedia.or...|\n",
      "|            →‎Roster|  safe|    Debartolo2917|{{short descripti...|{{short descripti...|2018 Kentucky Wil...|//en.wikipedia.or...|\n",
      "|             grammar|  safe|          Znagy88|{{Infobox settlem...|{{Infobox settlem...|         Karancsalja|//en.wikipedia.or...|\n",
      "|                    |unsafe|     42.61.139.39|{{about|the actre...|{{about|the actre...|        Kim Yoo-jung|//en.wikipedia.or...|\n",
      "|- 5 categories us...|  safe|       Richhoncho|{{other uses}}\n",
      "{{...|{{other uses}}\n",
      "{{...|         Peter Piper|//en.wikipedia.or...|\n",
      "|→‎Bibliography:am...|  safe|           FDW777|{{redirect|PIRA}}...|{{redirect|PIRA}}...|Provisional Irish...|//en.wikipedia.or...|\n",
      "|Fixed citation an...|  safe|          Nempnet|{{Use dmy dates|d...|{{Use dmy dates|d...|Caledonian Railwa...|//en.wikipedia.or...|\n",
      "|→‎Appearances in ...|  safe|       Nedrutland|{{Ibid|date=Augus...|{{Ibid|date=Augus...|Department of Ang...|//en.wikipedia.or...|\n",
      "|  →‎Season 17 (2020)|  safe|            Eriru|{{short descripti...|{{short descripti...|List of Ridiculou...|//en.wikipedia.or...|\n",
      "|                    |  safe|        Happyomen|{{Infobox artist ...|{{Infobox artist ...|Band of Horses di...|//en.wikipedia.or...|\n",
      "|external link rem...|  safe|          Manatoc|{{Infobox musical...|{{Infobox musical...|         Todd Tobias|//en.wikipedia.or...|\n",
      "|          →‎Examples|  safe|       Artoria2e5|{{lowercase|title...|{{lowercase|title...|               Xargs|//en.wikipedia.or...|\n",
      "|→‎top:replaced: A...|  safe|Chris the speller|{{Short descripti...|{{Short descripti...| James T. Vaughn Jr.|//en.wikipedia.or...|\n",
      "|                    |  safe|    Dangerousalex|{{Infobox album\n",
      "|...|{{Infobox album\n",
      "|...|Sexorcism (Brooke...|//en.wikipedia.or...|\n",
      "|(Reverted 1 edit ...|unsafe|      Classicwiki|{{short descripti...|{{short descripti...|Constitutional mo...|//en.wikipedia.or...|\n",
      "|(reference is ina...|unsafe|         Jhillx01|{{For|the 2015 Sw...|{{For|the 2015 Sw...|           Girl Lost|//en.wikipedia.or...|\n",
      "|                    |  safe|          Lugnuts|{{short descripti...|{{short descripti...|   The Flight (film)|//en.wikipedia.or...|\n",
      "|(→‎Other famous B...|unsafe|          A.nix29|{{short descripti...|{{short descripti...|     Barbary pirates|//en.wikipedia.or...|\n",
      "+--------------------+------+-----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PUT ALL FILES TO ONE JSON DATAFRAME\n",
    "\n",
    "df = spark.read.json(sc.textFile(','.join(files_list)))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- comment: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- name_user: string (nullable = true)\n",
      " |-- text_new: string (nullable = true)\n",
      " |-- text_old: string (nullable = true)\n",
      " |-- title_page: string (nullable = true)\n",
      " |-- url_page: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SHOW SCHEMA OF THE DATASET\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| label|count|\n",
      "+------+-----+\n",
      "|  safe| 1405|\n",
      "|unsafe|  233|\n",
      "|vandal|   27|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DISTRIBUTION OF LABELS IN THE DATASET\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df.groupBy(\"label\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             comment| label|    name_user|            text_new|            text_old|          title_page|            url_page|\n",
      "+--------------------+------+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|→‎Parodies and co...|  safe| InedibleHulk|{{short descripti...|{{short descripti...|Smells Like Teen ...|//en.wikipedia.or...|\n",
      "|   →‎Literary career|  safe|     Coingeek|{{Infobox person\n",
      "...|{{Infobox person\n",
      "...|     Q. David Bowers|//en.wikipedia.or...|\n",
      "|            (Added.)|unsafe|AndersonL7333|{{unreferenced|da...|{{unreferenced|da...|National Front (I...|//en.wikipedia.or...|\n",
      "|→‎top:added short...|  safe|  Lepricavark|{{short descripti...|{{Infobox basebal...|          Amos Cross|//en.wikipedia.or...|\n",
      "|                    |unsafe|12.235.76.177|{{For|other battl...|{{For|other battl...|Battle of San Jac...|//en.wikipedia.or...|\n",
      "+--------------------+------+-------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO SAMPLE FROM DATAFRAME (USE SAMPLEBY TO SPECIFY SAMPLED FRACTIONS OF EACH CLASS)\n",
    "#import pyspark.sql\n",
    "#df_sample = df.sample(False, fraction = 0.1, seed = 100)\n",
    "#df_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             comment| label|        name_user|            text_new|            text_old|          title_page|            url_page|          difference|\n",
      "+--------------------+------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|→‎Preseason All-S...|  safe|    Debartolo2917|{{short descripti...|{{short descripti...|2019 Kentucky Wil...|//en.wikipedia.or...|                 Jr.|\n",
      "|→‎Churches:Fixing...|  safe|           BD2412|{{short descripti...|{{short descripti...|List of churches ...|//en.wikipedia.or...|       Augustinians||\n",
      "|→‎Churches holdin...|  safe|           BD2412|[[File:StPaulsCat...|[[File:StPaulsCat...|List of churches ...|//en.wikipedia.or...|                   s|\n",
      "|            →‎Roster|  safe|    Debartolo2917|{{short descripti...|{{short descripti...|2018 Kentucky Wil...|//en.wikipedia.or...|      Jr. Jr.|link=y|\n",
      "|             grammar|  safe|          Znagy88|{{Infobox settlem...|{{Infobox settlem...|         Karancsalja|//en.wikipedia.or...|must s probably b...|\n",
      "|                    |unsafe|     42.61.139.39|{{about|the actre...|{{about|the actre...|        Kim Yoo-jung|//en.wikipedia.or...|                  39|\n",
      "|- 5 categories us...|  safe|       Richhoncho|{{other uses}}\n",
      "{{...|{{other uses}}\n",
      "{{...|         Peter Piper|//en.wikipedia.or...|English-language ...|\n",
      "|→‎Bibliography:am...|  safe|           FDW777|{{redirect|PIRA}}...|{{redirect|PIRA}}...|Provisional Irish...|//en.wikipedia.or...|| last = Coogan |...|\n",
      "|Fixed citation an...|  safe|          Nempnet|{{Use dmy dates|d...|{{Use dmy dates|d...|Caledonian Railwa...|//en.wikipedia.or...|                   t|\n",
      "|→‎Appearances in ...|  safe|       Nedrutland|{{Ibid|date=Augus...|{{Ibid|date=Augus...|Department of Ang...|//en.wikipedia.or...|            (2009)y'|\n",
      "|  →‎Season 17 (2020)|  safe|            Eriru|{{short descripti...|{{short descripti...|List of Ridiculou...|//en.wikipedia.or...|0.27<ref>{{cite w...|\n",
      "|                    |  safe|        Happyomen|{{Infobox artist ...|{{Infobox artist ...|Band of Horses di...|//en.wikipedia.or...|[[enmark|IFPI D]]...|\n",
      "|external link rem...|  safe|          Manatoc|{{Infobox musical...|{{Infobox musical...|         Todd Tobias|//en.wikipedia.or...|  Italian vocalis...|\n",
      "|          →‎Examples|  safe|       Artoria2e5|{{lowercase|title...|{{lowercase|title...|               Xargs|//en.wikipedia.or...|                  \n",
      "\n",
      "|\n",
      "|→‎top:replaced: A...|  safe|Chris the speller|{{Short descripti...|{{Short descripti...| James T. Vaughn Jr.|//en.wikipedia.or...|      AssocissociaJj|\n",
      "|                    |  safe|    Dangerousalex|{{Infobox album\n",
      "|...|{{Infobox album\n",
      "|...|Sexorcism (Brooke...|//en.wikipedia.or...|                [[]]|\n",
      "|(Reverted 1 edit ...|unsafe|      Classicwiki|{{short descripti...|{{short descripti...|Constitutional mo...|//en.wikipedia.or...|                 n a|\n",
      "|(reference is ina...|unsafe|         Jhillx01|{{For|the 2015 Sw...|{{For|the 2015 Sw...|           Girl Lost|//en.wikipedia.or...|>|ref1=<ref>https...|\n",
      "|                    |  safe|          Lugnuts|{{short descripti...|{{short descripti...|   The Flight (film)|//en.wikipedia.or...|         1720|df=yes|\n",
      "|(→‎Other famous B...|unsafe|          A.nix29|{{short descripti...|{{short descripti...|     Barbary pirates|//en.wikipedia.or...|\n",
      "* [[Kemal Reis]]...|\n",
      "+--------------------+------+-----------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DIFFERENCE BETWEEN OLD AND NEW TEXT\n",
    "\n",
    "import difflib\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def make_diff(old, new):\n",
    "    diff = difflib.ndiff(old, new)\n",
    "    delta = ''.join(x[2:] for x in diff if x.startswith('- ') or x.startswith('+'))\n",
    "    return delta\n",
    "\n",
    "#convert to a UDF Function and get difference between columns\n",
    "udfmake_diff = F.udf(make_diff, StringType())\n",
    "df_difference = df.withColumn(\"difference\", lit(udfmake_diff(\"text_old\", \"text_new\")))\n",
    "df_difference.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE COLUMN NAME FROM 'LABEL' TO 'LABEL_STRING' -- LATER WE WILL CONVERT LABEL_STRING TO NUMERICAL VALUES AND GIVE THE COLUMN NAME 'LABEL'\n",
    "\n",
    "df_wd = df_difference.withColumnRenamed('label', 'label_string')\n",
    "#df_wd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "##\n",
    "## NOTE !!!!!!\n",
    "##\n",
    "## CHOOSE TO RUN ONLY ONE OF THE FOLLOWING TWO CELLS\n",
    "## DO NOT RUN THEM BOTH\n",
    "##\n",
    "##\n",
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 1175\n",
      "Test Dataset Count: 490\n",
      "Distribution of labels in train set:\n",
      "+------------+-----+\n",
      "|label_string|count|\n",
      "+------------+-----+\n",
      "|        safe|  996|\n",
      "|      unsafe|  163|\n",
      "|      vandal|   16|\n",
      "+------------+-----+\n",
      "\n",
      "Distribution of labels in test set:\n",
      "+------------+-----+\n",
      "|label_string|count|\n",
      "+------------+-----+\n",
      "|        safe|  409|\n",
      "|      unsafe|   70|\n",
      "|      vandal|   11|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EITHER RUN THIS CELL OR THE NEXT CELL (BUT NOT BOTH)\n",
    "# THIS IS RUN ONLY TO TEST MODEL ACCURACY AND TUNE THE MODEL -- SKIP TO TRAIN MODEL ON THE WHOLE DATASET\n",
    "\n",
    "# split dataset to train and test sets\n",
    "\n",
    "(trainingData, testData) = df_wd.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))\n",
    "\n",
    "\n",
    "print(\"Distribution of labels in train set:\")\n",
    "trainingData.groupBy(\"label_string\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()\n",
    "\n",
    "print(\"Distribution of labels in test set:\")\n",
    "testData.groupBy(\"label_string\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EITHER RUN THIS CELL OR THE CELL ABOVE (BUT NOT BOTH)\n",
    "# THIS IS RUN TO TRAIN SELECTED MODEL ON THE ENTIRE DATASET\n",
    "\n",
    "trainingData = df_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPUTE NaN'S INSTEAD OF EMPTY STRINGS\n",
    "\n",
    "#def processMissingCategory(s):\n",
    "#    if s == \"\":\n",
    "#        return \"NaN\"\n",
    "#    else:\n",
    "#        return s\n",
    "\n",
    "#udfprocessMissingCategory = F.udf(processMissingCategory, StringType())\n",
    "#df_nona = df_difference.withColumn('comment', lit(udfprocessMissingCategory('comment')))\n",
    "#df_nona.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZING 'DIFFERENCE' COLUMN AND REMOVING CERTAIN STOP WORDS\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "\n",
    "# tokenize 'difference' column\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"difference\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "regexTokenizer_comment = RegexTokenizer(inputCol=\"comment\", outputCol=\"words_comment\", pattern=\"\\\\W\")\n",
    "\n",
    "# remove stop words\n",
    "stop_words = [\"http\",\"https\", \"a\", \"an\", \"the\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"and\", \"any\", \"are\", \"aren't\", \"as\", \"at\", \"be\", \"because\",\n",
    "              \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can't\", \"cannot\", \"could\", \"couldn't\", \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \n",
    "              \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\",\n",
    "              \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\",\n",
    "              \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"mustn't\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \"once\", \"only\",\n",
    "              \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"shan't\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"shouldn't\",\n",
    "              \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\",\n",
    "              \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\",\n",
    "              \"were\", \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"won't\",\n",
    "              \"would\", \"wouldn't\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\"] \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(stop_words)\n",
    "stopwordsRemover_comment = StopWordsRemover(inputCol=\"words_comment\", outputCol=\"filtered_comment\").setStopWords(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT 'LABEL_STRING' TO NUMERIC COLUMN 'LABEL'\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# index categorical variable\n",
    "label_stringIdx = StringIndexer(inputCol = \"label_string\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "##\n",
    "## NOTE !!!!!!\n",
    "##\n",
    "## CHOOSE TO RUN ONLY ONE OF THE FOLLOWING TWO CELLS\n",
    "## DO NOT RUN THEM BOTH\n",
    "##\n",
    "##\n",
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF AND PIPELINE -- WITHOUT 'COMMENT'\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# do TF-IDF embeding\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# define the pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF AND PIPELINE -- WITH 'COMMENT'\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# do TF-IDF embeding\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features_diff\")\n",
    "\n",
    "hashingTF_comment = HashingTF(inputCol=\"filtered_comment\", outputCol=\"rawFeatures_comment\", numFeatures=10000)\n",
    "idf_comment = IDF(inputCol=\"rawFeatures_comment\", outputCol=\"features_comment\")\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"features_comment\", \"features_diff\"], outputCol=\"features\")\n",
    "\n",
    "# define the pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, regexTokenizer_comment, stopwordsRemover, stopwordsRemover_comment, hashingTF, hashingTF_comment, idf, idf_comment, \n",
    "                            assembler, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT PIPELINE ON TRAIN DATASET\n",
    "\n",
    "pipelineFit = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE PIPELINE\n",
    "\n",
    "#outpath = 'C:/Pr/AA_Big_Data/Assignment_3/spark/models/without_comment/tfidf'\n",
    "#outpath = 'C:/Users/Aistuxxe/Documents/spark/models/with_comment/tfidf'\n",
    "#pipelineFit.write().overwrite().save(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# USE PIPELINE ON TRAIN DATASET\n",
    "\n",
    "dataset_tr = pipelineFit.transform(trainingData)\n",
    "\n",
    "# without 'comment':\n",
    "#dataset_tr.select(\"difference\",\"words\",\"filtered\",\"rawFeatures\",\"features\", \"label\").show()\n",
    "\n",
    "# with 'comment':\n",
    "#dataset_tr.select(\"difference\",\"words\",\"filtered\",\"rawFeatures\",\"features_diff\", \"label\").show()\n",
    "#dataset_tr.select(\"comment\",\"words_comment\",\"filtered_comment\",\"rawFeatures_comment\",\"features_comment\", \"label\").show()\n",
    "#dataset_tr.select(\"features_diff\", \"features_comment\", \"features\", \"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE PIPELINE ON TEST DATASET (ONLY DO IF THE SPLIT BETWEEN TRAIN AND TEST SETS WAS PERFORMED)\n",
    "\n",
    "dataset_test = pipelineFit.transform(testData)\n",
    "\n",
    "# without 'comment':\n",
    "#dataset_test.select(\"difference\",\"words\",\"filtered\",\"rawFeatures\",\"features\", \"label\").show()\n",
    "\n",
    "# with 'comment':\n",
    "#dataset_test.select(\"difference\",\"words\",\"filtered\",\"rawFeatures\",\"features_diff\", \"label\").show()\n",
    "#dataset_test.select(\"comment\",\"words_comment\",\"filtered_comment\",\"rawFeatures_comment\",\"features_comment\", \"label\").show()\n",
    "#dataset_test.select(\"features_diff\", \"features_comment\", \"features\", \"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE WILL USE LOGISTIC REGRESSION FOR PREDICTING\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "##\n",
    "## NOTE !!!!!!\n",
    "##\n",
    "## THE FOLLOWING PART IS FOR TESTING MODEL ACCURACY, MODEL TUNING AND DECIDING WHICH MODEL TO USE\n",
    "## USE IT ONLY IF YOU SPLIT INITIAL DATASET TO TRAIN AND TEST SETS\n",
    "## IN ORDER TO USE THE WHOLE DATASET AND TRAIN THE FINAL MODEL, SKIP UNTIL THE NEXT NOTICE\n",
    "##\n",
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+--------------------+-----+----------+\n",
      "|             comment|          difference|label_string|         probability|label|prediction|\n",
      "+--------------------+--------------------+------------+--------------------+-----+----------+\n",
      "|→‎Preseason All-S...|                 Jr.|        safe|[0.98796644742399...|  0.0|       0.0|\n",
      "|→‎Churches:Fixing...|       Augustinians||        safe|[0.99981918066159...|  0.0|       0.0|\n",
      "|→‎Churches holdin...|                   s|        safe|[0.96516371365320...|  0.0|       0.0|\n",
      "|             grammar|must s probably b...|        safe|[0.99999989444544...|  0.0|       0.0|\n",
      "|                    |                  39|      unsafe|[0.81871781744510...|  1.0|       0.0|\n",
      "|Fixed citation an...|                   t|        safe|[0.97667989166664...|  0.0|       0.0|\n",
      "|→‎Appearances in ...|            (2009)y'|        safe|[0.96484976127255...|  0.0|       0.0|\n",
      "|  →‎Season 17 (2020)|0.27<ref>{{cite w...|        safe|[0.99999997321040...|  0.0|       0.0|\n",
      "|                    |[[enmark|IFPI D]]...|        safe|[0.99998534612742...|  0.0|       0.0|\n",
      "|external link rem...|  Italian vocalis...|        safe|[0.99999917562419...|  0.0|       0.0|\n",
      "|          →‎Examples|                  \n",
      "\n",
      "|        safe|[0.96392159880838...|  0.0|       0.0|\n",
      "|                    |                [[]]|        safe|[0.96392159880838...|  0.0|       0.0|\n",
      "|(reference is ina...|>|ref1=<ref>https...|      unsafe|[2.96919547739427...|  1.0|       1.0|\n",
      "|                    |         1720|df=yes|        safe|[0.99451425989106...|  0.0|       0.0|\n",
      "|→‎Rodnover spirit...|        21twenty-fir|        safe|[0.99995292252355...|  0.0|       0.0|\n",
      "|dashes, fixed usi...|                  -–|        safe|[0.96392159880838...|  0.0|       0.0|\n",
      "|                    |<br>This is unlik...|        safe|[0.99999976972822...|  0.0|       0.0|\n",
      "|      →‎top:clean up|,    motor failur...|        safe|[0.99999764896354...|  0.0|       0.0|\n",
      "|→‎top:Fixinglinks...|       Augustinians||        safe|[0.99981918066159...|  0.0|       0.0|\n",
      "|fixingdisallowed ...|}}\n",
      "{{DISPLAYTITLE...|        safe|[0.99999582429195...|  0.0|       0.0|\n",
      "+--------------------+--------------------+------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+------------+--------------------+-----+----------+\n",
      "|             comment|          difference|label_string|         probability|label|prediction|\n",
      "+--------------------+--------------------+------------+--------------------+-----+----------+\n",
      "|            →‎Roster|      Jr. Jr.|link=y|        safe|[0.99982250713564...|  0.0|       0.0|\n",
      "|- 5 categories us...|English-language ...|        safe|[0.99784492200796...|  0.0|       0.0|\n",
      "|→‎Bibliography:am...|| last = Coogan |...|        safe|[3.08723099337592...|  0.0|       1.0|\n",
      "|→‎top:replaced: A...|      AssocissociaJj|        safe|[0.96392159880838...|  0.0|       0.0|\n",
      "|(Reverted 1 edit ...|                 n a|      unsafe|[0.97846154155299...|  1.0|       0.0|\n",
      "|(→‎Other famous B...|\n",
      "* [[Kemal Reis]]...|      unsafe|[3.83469838678021...|  1.0|       1.0|\n",
      "|            →‎Roster|          Jr.|link=y|        safe|[0.99952474100575...|  0.0|       0.0|\n",
      "|                    |    MaySeptember1820|        safe|[0.83199681656690...|  0.0|       0.0|\n",
      "|(→‎Film and telev...|}}</ref>\n",
      "* ''West...|      unsafe|[0.99999794604217...|  1.0|       0.0|\n",
      "|    →‎Chondrichthyes|\n",
      "|-\n",
      "|}\n",
      "\n",
      "====Sarco...|        safe|[4.94886003980569...|  0.0|       1.0|\n",
      "|→‎17th century:Fi...|       s|Augustinian|        safe|[0.99991596049109...|  0.0|       0.0|\n",
      "|Disambiguating li...|Livingston, West ...|        safe|[0.97649643252697...|  0.0|       0.0|\n",
      "|→‎Alphabetical li...|           (instr.) |        safe|[0.99103481000737...|  0.0|       0.0|\n",
      "|→‎References:Adde...|{{Portal|Cornwall}}\n",
      "|        safe|[0.99993786099171...|  0.0|       0.0|\n",
      "|→‎Release history...|                    |        safe|[0.96392159880838...|  0.0|       0.0|\n",
      "|        (→‎Behavior)|They-->also feed ...|      unsafe|[2.03892452972407...|  1.0|       1.0|\n",
      "|              layout|upright=0.75|upri...|        safe|[0.02368480812194...|  0.0|       2.0|\n",
      "|Fixed citation an...|                   t|        safe|[0.97667989166664...|  0.0|       0.0|\n",
      "|→‎External links:...|\n",
      "{{Portal|Cornwall}}|        safe|[0.99993786099171...|  0.0|       0.0|\n",
      "|    Misspelling fix.|                  uo|        safe|[0.96990842653149...|  0.0|       0.0|\n",
      "+--------------------+--------------------+------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION WITHOUT WEIGHTS\n",
    "\n",
    "# fit logistic regression\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20)\n",
    "lrModel = lr.fit(dataset_tr)\n",
    "\n",
    "\n",
    "# predict train and test sets\n",
    "predict_train = lrModel.transform(dataset_tr)\n",
    "predict_test = lrModel.transform(dataset_test)\n",
    "\n",
    "#comment if not used\n",
    "predict_train.select(\"comment\", \"difference\",\"label_string\",\"probability\",\"label\",\"prediction\").show()\n",
    "predict_test.select(\"comment\", \"difference\",\"label_string\",\"probability\",\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE LOGISTIC REGRESSION WITHOUT WEIGHTS\n",
    "\n",
    "outpath = 'C:/Pr/AA_Big_Data/Assignment_3/spark/models/without_comment/logistic_regression'\n",
    "#outpath = 'C:/Users/Aistuxxe/Documents/spark/models/with_comment/logistic_regression'\n",
    "\n",
    "lrModel.write().overwrite().save(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD LOGISTIC REGRESSION WITHOUT WEIGHTS\n",
    "\n",
    "#from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "#LogisticRegressionModel.load('C:/Users/Aistuxxe/Documents/spark/models/without_comment/logistic_regression')\n",
    "#LogisticRegressionModel.load('C:/Users/Aistuxxe/Documents/spark/models/with_comment/logistic_regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall accuracy train set: 0.9846808510638297\n",
      "overall accuracy test set: 0.8122448979591836\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE OVERALL MODEL ACCURACY FOR TRAIN AND TEST SETS\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol = \"label\", metricName=\"accuracy\")\n",
    "\n",
    "print(\"overall accuracy train set: %s\" % (evaluator.evaluate(predict_train)))\n",
    "print(\"overall accuracy test set: %s\" % (evaluator.evaluate(predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy for label 0 (safe): 0.9949799196787149\n",
      "training accuracy for label 1 (unsafe): 0.9202453987730062\n",
      "training accuracy for label 2 (vandal): 1.0\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE MODEL ACCURACY FOR TRAIN SET BY LABEL\n",
    "\n",
    "zeros = predict_train.where(\"label == 0\").select(\"label\", \"prediction\")\n",
    "print(\"training accuracy for label 0 (safe): %s\" % (evaluator.evaluate(zeros)))\n",
    "\n",
    "ones = predict_train.where(\"label == 1\").select(\"label\", \"prediction\")\n",
    "print(\"training accuracy for label 1 (unsafe): %s\" % (evaluator.evaluate(ones)))\n",
    "\n",
    "twos = predict_train.where(\"label == 2\").select(\"label\", \"prediction\")\n",
    "print(\"training accuracy for label 2 (vandal): %s\" % (evaluator.evaluate(twos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy for label 0 (safe): 0.9266503667481663\n",
      "training accuracy for label 1 (unsafe): 0.2714285714285714\n",
      "training accuracy for label 2 (vandal): 0.0\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE MODEL ACCURACY FOR TEST SET BY LABEL\n",
    "\n",
    "zeros_test = predict_test.where(\"label == 0\").select(\"label\", \"prediction\")\n",
    "print(\"training accuracy for label 0 (safe): %s\" % (evaluator.evaluate(zeros_test)))\n",
    "\n",
    "ones_test = predict_test.where(\"label == 1\").select(\"label\", \"prediction\")\n",
    "print(\"training accuracy for label 1 (unsafe): %s\" % (evaluator.evaluate(ones_test)))\n",
    "\n",
    "twos_test = predict_test.where(\"label == 2\").select(\"label\", \"prediction\")\n",
    "print(\"training accuracy for label 2 (vandal): %s\" % (evaluator.evaluate(twos_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate by label:\n",
      "label 0: 0.0\n",
      "label 1: 0.0\n",
      "label 2: 0.0\n",
      "True positive rate by label:\n",
      "label 0: 1.0\n",
      "label 1: 1.0\n",
      "label 2: 1.0\n",
      "Precision by label:\n",
      "label 0: 1.0\n",
      "label 1: 1.0\n",
      "label 2: 1.0\n",
      "Recall by label:\n",
      "label 0: 1.0\n",
      "label 1: 1.0\n",
      "label 2: 1.0\n"
     ]
    }
   ],
   "source": [
    "# STATISTICS ON TRAINING DATA\n",
    "\n",
    "#trainingSummary = lrModel.summary\n",
    "\n",
    "# for multiclass, we can inspect metrics on a per-label basis\n",
    "#print(\"False positive rate by label:\")\n",
    "#for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "#    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "#print(\"True positive rate by label:\")\n",
    "#for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "#    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "#print(\"Precision by label:\")\n",
    "#for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "#    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "#print(\"Recall by label:\")\n",
    "#for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "#    print(\"label %d: %s\" % (i, rec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-FOLD CROSS-VALIDATION\n",
    "\n",
    "#from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "#lr_cv = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0.5)\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "#paramGrid = (ParamGridBuilder()\n",
    "#             .addGrid(lr_cv.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "#             .addGrid(lr_cv.elasticNetParam, [0.0, 0.5, 1.0]) # Elastic Net Parameter (Ridge = 0)\n",
    "#             .build())\n",
    "\n",
    "#evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol = \"label\", metricName=\"accuracy\")\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "#cv = CrossValidator(estimator=lr_cv, \\\n",
    "#                    estimatorParamMaps=paramGrid, \\\n",
    "#                    evaluator=evaluator, \\\n",
    "#                    numFolds=5)\n",
    "#cvModel = cv.fit(dataset_tr)\n",
    "\n",
    "#predict_train_cv = cvModel.transform(dataset_tr)\n",
    "#predict_test_cv = cvModel.transform(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Param (regParam):  0.1\n"
     ]
    }
   ],
   "source": [
    "#print ('Best Param (regParam): ', best._java_obj.getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Param (elasticNetParam):  0.5\n"
     ]
    }
   ],
   "source": [
    "#print ('Best Param (elasticNetParam): ', best._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE CV MODEL WITHOUT WEIGHTS\n",
    "\n",
    "#outpath = 'C:/Users/Aistuxxe/Documents/spark/models/without_comment/logistic_regression_cv'\n",
    "#outpath = 'C:/Users/Aistuxxe/Documents/spark/models/with_comment/logistic_regression_cv'\n",
    "\n",
    "#cvModel.write().overwrite().save(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CV MODEL WITHOUT WEIGHTS\n",
    "\n",
    "#CrossValidator.load('C:/Users/Aistuxxe/Documents/spark/models/without_comment/logistic_regression_cv')\n",
    "#CrossValidator.load('C:/Users/Aistuxxe/Documents/spark/models/with_comment/logistic_regression_cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE OVERALL CV MODEL ACCURACY FOR TRAIN AND TEST SETS\n",
    "\n",
    "#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol = \"label\", metricName=\"accuracy\")\n",
    "\n",
    "#print(\"overall accuracy train set: %s\" % (evaluator.evaluate(predict_train_cv)))\n",
    "#print(\"overall accuracy test set: %s\" % (evaluator.evaluate(predict_test_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE CV MODEL ACCURACY FOR TRAIN SET BY LABEL\n",
    "\n",
    "#zeros_cv = predict_train_cv.where(\"label == 0\").select(\"label\", \"prediction\")\n",
    "#print(\"training accuracy for label 0 (safe): %s\" % (evaluator.evaluate(zeros_cv)))\n",
    "\n",
    "#ones_cv = predict_train_cv.where(\"label == 1\").select(\"label\", \"prediction\")\n",
    "#print(\"training accuracy for label 1 (unsafe): %s\" % (evaluator.evaluate(ones_cv)))\n",
    "\n",
    "#twos_cv = predict_train_cv.where(\"label == 2\").select(\"label\", \"prediction\")\n",
    "#print(\"training accuracy for label 2 (vandal): %s\" % (evaluator.evaluate(twos_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE CV MODEL ACCURACY FOR TEST SET BY LABEL\n",
    "\n",
    "#zeros_test_cv = predict_test_cv.where(\"label == 0\").select(\"label\", \"prediction\")\n",
    "#print(\"training accuracy for label 0 (safe): %s\" % (evaluator.evaluate(zeros_test_cv)))\n",
    "\n",
    "#ones_test_cv = predict_test_cv.where(\"label == 1\").select(\"label\", \"prediction\")\n",
    "#print(\"training accuracy for label 1 (unsafe): %s\" % (evaluator.evaluate(ones_test_cv)))\n",
    "\n",
    "#twos_test_cv = predict_test_cv.where(\"label == 2\").select(\"label\", \"prediction\")\n",
    "#print(\"training accuracy for label 2 (vandal): %s\" % (evaluator.evaluate(twos_test_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of 0s is 0.8476595744680852\n",
      "The percentage of 1s is 0.13872340425531915\n",
      "The percentage of 2s is 0.013617021276595698\n",
      "The weight of class safe is 1.179718875502008\n",
      "The weight of class unsafe is 7.208588957055214\n",
      "The weight of class vandal is 73.43750000000026\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION WITH WEIGHTS -- CALCULATING WEIGHTS\n",
    "\n",
    "dataset_tr_size = float(dataset_tr.select(\"label\").count())\n",
    "\n",
    "numZeros = dataset_tr.select(\"label\").where('label == 0').count()\n",
    "perZeros = (float(numZeros)/float(dataset_tr_size))\n",
    "\n",
    "numOnes = dataset_tr.select(\"label\").where('label == 1').count()\n",
    "perOnes = (float(numOnes)/float(dataset_tr_size))\n",
    "\n",
    "perTwos = 1-perOnes-perZeros\n",
    "\n",
    "print('The percentage of 0s is {}'.format(perZeros))\n",
    "print('The percentage of 1s is {}'.format(perOnes))\n",
    "print('The percentage of 2s is {}'.format(perTwos))\n",
    "\n",
    "weight0 = 1/(perZeros)\n",
    "weight1 = 1/(perOnes)\n",
    "weight2 = 1/(perTwos)\n",
    "\n",
    "print('The weight of class safe is {}'.format(weight0))\n",
    "print('The weight of class unsafe is {}'.format(weight1))\n",
    "print('The weight of class vandal is {}'.format(weight2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION WITH WEIGHTS -- CALCULATING WEIGHTS\n",
    "\n",
    "#dataset_tr_size = float(dataset_tr.select(\"label\").count())\n",
    "\n",
    "#numZeros = dataset_tr.select(\"label\").where('label == 0').count()\n",
    "#perZeros = (float(numZeros)/float(dataset_tr_size))\n",
    "\n",
    "#numOnes = dataset_tr.select(\"label\").where('label == 1').count()\n",
    "#perOnes = (float(numOnes)/float(dataset_tr_size))\n",
    "\n",
    "#perTwos = 1-perOnes-perZeros\n",
    "\n",
    "#print('The percentage of 0s is {}'.format(perZeros))\n",
    "#print('The percentage of 1s is {}'.format(perOnes))\n",
    "#print('The percentage of 2s is {}'.format(perTwos))\n",
    "\n",
    "#weight0 = 1/(perZeros/perTwos)\n",
    "#weight1 = 1/(perOnes/perTwos)\n",
    "#weight2 = 1\n",
    "\n",
    "#print('The weight of class safe is {}'.format(weight0))\n",
    "#print('The weight of class unsafe is {}'.format(weight1))\n",
    "#print('The weight of class vandal is {}'.format(weight2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|label|     classWeights|\n",
      "+-----+-----------------+\n",
      "|  0.0|1.179718875502008|\n",
      "|  0.0|1.179718875502008|\n",
      "|  0.0|1.179718875502008|\n",
      "|  0.0|1.179718875502008|\n",
      "|  1.0|7.208588957055214|\n",
      "|  0.0|1.179718875502008|\n",
      "|  0.0|1.179718875502008|\n",
      "|  0.0|1.179718875502008|\n",
      "|  0.0|1.179718875502008|\n",
      "|  0.0|1.179718875502008|\n",
      "|  0.0|1.179718875502008|\n",
      "|  0.0|1.179718875502008|\n",
      "|  1.0|7.208588957055214|\n",
      "|  0.0|1.179718875502008|\n",
      "|  0.0|1.179718875502008|\n",
      "|  0.0|1.179718875502008|\n",
      "|  0.0|1.179718875502008|\n",
      "|  0.0|1.179718875502008|\n",
      "|  0.0|1.179718875502008|\n",
      "|  0.0|1.179718875502008|\n",
      "+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION WITH WEIGHTS -- ADDING WEIGHT COLUMN\n",
    "\n",
    "dataset_tr = dataset_tr.withColumn(\"classWeights\", F.when(dataset_tr.label == 0, weight0).when(dataset_tr.label == 1, weight1).otherwise(weight2))\n",
    "dataset_tr.select(\"label\",\"classWeights\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION WITH WEIGHTS -- TRAINING AND PREDICTING\n",
    "\n",
    "lr_w = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", weightCol=\"classWeights\", maxIter=20)\n",
    "lrModel_w = lr_w.fit(dataset_tr)\n",
    "\n",
    "predict_train_w = lrModel_w.transform(dataset_tr)\n",
    "predict_test_w = lrModel_w.transform(dataset_test)\n",
    "#predict_train_w.select(\"difference\",\"label_string\",\"probability\",\"label\",\"prediction\").show()\n",
    "#predict_test_w.select(\"difference\",\"label_string\",\"probability\",\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE LOGISTIC REGRESSION WITH WEIGHTS\n",
    "\n",
    "outpath = 'C:/Pr/AA_Big_Data/Assignment_3/spark/models/without_comment/logistic_regression_with_weights'\n",
    "#outpath = 'C:/Users/Aistuxxe/Documents/spark/models/with_comment/logistic_regression_with_weights'\n",
    "\n",
    "lrModel_w.write().overwrite().save(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD LOGISTIC REGRESSION WITH WEIGHTS\n",
    "\n",
    "#from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "#LogisticRegressionModel.load('C:/Users/Aistuxxe/Documents/spark/models/without_comment/logistic_regression_with_weights')\n",
    "#LogisticRegressionModel.load('C:/Users/Aistuxxe/Documents/spark/models/with_comment/logistic_regression_with_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE MODEL ACCURACY FOR TRAIN AND TEST SETS\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol = \"label\", metricName=\"accuracy\")\n",
    "\n",
    "#print(\"overall accuracy train set: %s\" % (evaluator.evaluate(predict_train_w)))\n",
    "#print(\"overall accuracy test set: %s\" % (evaluator.evaluate(predict_test_w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy for label 0 (safe): 0.9899598393574297\n",
      "training accuracy for label 1 (unsafe): 0.9570552147239264\n",
      "training accuracy for label 2 (vandal): 1.0\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE MODEL ACCURACY FOR TRAIN SET BY LABEL\n",
    "\n",
    "zeros_w = predict_train_w.where(\"label == 0\").select(\"label\", \"prediction\")\n",
    "print(\"training accuracy for label 0 (safe): %s\" % (evaluator.evaluate(zeros_w)))\n",
    "\n",
    "ones_w = predict_train_w.where(\"label == 1\").select(\"label\", \"prediction\")\n",
    "print(\"training accuracy for label 1 (unsafe): %s\" % (evaluator.evaluate(ones_w)))\n",
    "\n",
    "twos_w = predict_train_w.where(\"label == 2\").select(\"label\", \"prediction\")\n",
    "print(\"training accuracy for label 2 (vandal): %s\" % (evaluator.evaluate(twos_w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy for label 0 (safe): 0.9535452322738386\n",
      "testing accuracy for label 1 (unsafe): 0.24285714285714285\n",
      "testing accuracy for label 2 (vandal): 0.0\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE MODEL ACCURACY FOR TEST SET BY LABEL\n",
    "\n",
    "zeros_test_w = predict_test_w.where(\"label == 0\").select(\"label\", \"prediction\")\n",
    "print(\"testing accuracy for label 0 (safe): %s\" % (evaluator.evaluate(zeros_test_w)))\n",
    "\n",
    "ones_test_w = predict_test_w.where(\"label == 1\").select(\"label\", \"prediction\")\n",
    "print(\"testing accuracy for label 1 (unsafe): %s\" % (evaluator.evaluate(ones_test_w)))\n",
    "\n",
    "twos_test_w = predict_test_w.where(\"label == 2\").select(\"label\", \"prediction\")\n",
    "print(\"testing accuracy for label 2 (vandal): %s\" % (evaluator.evaluate(twos_test_w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "##\n",
    "## NOTE !!!!!!\n",
    "##\n",
    "## THE FOLLOWING PART IS FOR TRAINING THE BEST MODEL ON THE WHOLE DATASET\n",
    "##\n",
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_string = predict_train.withColumn(\"prediction_string\", F.when(predict_train.label == 0, \"safe\").when(predict_train.label == 1, \"unsafe\").otherwise(\"vandal\"))\n",
    "predictions_string.select(\"comment\", \"difference\", \"label\",\"prediction\", \"label_string\", \"predicted_string\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
