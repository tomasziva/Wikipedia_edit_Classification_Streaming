{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Tomas-PC:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7c32248>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read RDDs\n",
    "import os\n",
    "rootdir = 'C:/Pr/AA_Big_Data/Assignment_3/spark/save_sub'\n",
    "\n",
    "files_list = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for name in files:\n",
    "        if \"part\" in name.lower() and not \".crc\" in name.lower():\n",
    "            #files_list.append( sc.textFile(os.path.join(subdir,name)) )\n",
    "            files_list.append(os.path.join(subdir,name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+---------------+--------------------+\n",
      "|             comment|label|           name_user|            text_new|            text_old|     title_page|            url_page|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+---------------+--------------------+\n",
      "|→‎Theatrical care...| safe|           John B123|{{short descripti...|{{short descripti...|Dorothea Jordan|//en.wikipedia.or...|\n",
      "|                    | safe|           TSUBAME98|{{BLP sources|dat...|{{BLP sources|dat...|      Kan Otake|//en.wikipedia.or...|\n",
      "|→‎top:hatnote not...| safe|             Uanfala|The '''Motozintle...|{{for|the languag...|  Motozintlecos|//en.wikipedia.or...|\n",
      "|Add image caption...| safe|         Robby.is.on|{{short descripti...|{{short descripti...|  Joseph Akpala|//en.wikipedia.or...|\n",
      "|Removed external ...| safe|Anthonygalluccisc...|{{wikt|Kemet|ke'm...|{{wikt|Kemet|ke'm...|          Kemet|//en.wikipedia.or...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(sc.textFile(','.join(files_list)))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comment', 'label', 'name_user', 'text_new', 'text_old', 'title_page', 'url_page']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check data structure\n",
    "print(df.columns)\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.toPandas().to_csv('C:/Pr/AA_Big_Data/Assignment_3/spark/csv/mycsv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define diff\n",
    "from difflib import unified_diff\n",
    "\n",
    "def make_diff(old, new):\n",
    "    return '\\n'.join([ l for l in unified_diff(old.split('\\n'), new.split('\\n')) if l.startswith('+') or l.startswith('-') ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                diff|\n",
      "+--------------------+\n",
      "|--- \n",
      "\n",
      "+++ \n",
      "\n",
      "-|ali...|\n",
      "|--- \n",
      "\n",
      "+++ \n",
      "\n",
      "-|ali...|\n",
      "|--- \n",
      "\n",
      "+++ \n",
      "\n",
      "-|ali...|\n",
      "|--- \n",
      "\n",
      "+++ \n",
      "\n",
      "-|ali...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#make diff column\n",
    "diff = make_diff(df.first().text_old, df.first().text_new)\n",
    "df_withdiff = df.withColumn(\"diff\", lit(diff))\n",
    "df_withdiff.select('diff').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change column\n",
    "df_wd = df_withdiff.withColumnRenamed('label', 'label_string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|             comment|label|         name_user|            text_new|            text_old|          title_page|            url_page|                diff|\n",
      "+--------------------+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                    | safe|      WikiMaster37|{{pp-vandalism|sm...|{{pp-vandalism|sm...|List of World Ser...|//en.wikipedia.or...|--- \n",
      "\n",
      "+++ \n",
      "\n",
      "-|ali...|\n",
      "|→‎Members of the ...| safe|Mr Serjeant Buzfuz|{{short descripti...|{{short descripti...|Gaspé (Province o...|//en.wikipedia.or...|--- \n",
      "\n",
      "+++ \n",
      "\n",
      "-|ali...|\n",
      "|         →‎Biography| safe|      SabellaAsher|{{Use dmy dates|d...|{{Use dmy dates|d...|        Sabine Huynh|//en.wikipedia.or...|--- \n",
      "\n",
      "+++ \n",
      "\n",
      "-|ali...|\n",
      "|→‎The English Pro...| safe|        Jenhawk777|[[Persecution]] i...|[[Persecution]] i...|History of Christ...|//en.wikipedia.or...|--- \n",
      "\n",
      "+++ \n",
      "\n",
      "-|ali...|\n",
      "+--------------------+-----+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#show top 5 rows\n",
    "#print(df_withdiff.columns)\n",
    "print(df_withdiff.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For model pipeline: tokenize diff column, remove stop words\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"diff\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# stop words\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "# bag of words count\n",
    "#do not use, this uses count vectors instead of TF-IDF!\n",
    "#countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|             comment|label_string|         name_user|            text_new|            text_old|          title_page|            url_page|                diff|               words|            filtered|            features|label|\n",
      "+--------------------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|                    |        safe|      WikiMaster37|{{pp-vandalism|sm...|{{pp-vandalism|sm...|List of World Ser...|//en.wikipedia.or...|--- \n",
      "\n",
      "+++ \n",
      "\n",
      "-|ali...|[align, left, sty...|[align, left, sty...|(73,[0,1,2,3,4,5,...|  0.0|\n",
      "|→‎Members of the ...|        safe|Mr Serjeant Buzfuz|{{short descripti...|{{short descripti...|Gaspé (Province o...|//en.wikipedia.or...|--- \n",
      "\n",
      "+++ \n",
      "\n",
      "-|ali...|[align, left, sty...|[align, left, sty...|(73,[0,1,2,3,4,5,...|  0.0|\n",
      "|         →‎Biography|        safe|      SabellaAsher|{{Use dmy dates|d...|{{Use dmy dates|d...|        Sabine Huynh|//en.wikipedia.or...|--- \n",
      "\n",
      "+++ \n",
      "\n",
      "-|ali...|[align, left, sty...|[align, left, sty...|(73,[0,1,2,3,4,5,...|  0.0|\n",
      "+--------------------+------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#index categorical variable\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "label_stringIdx = StringIndexer(inputCol = \"label_string\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 3\n",
      "Test Dataset Count: 1\n"
     ]
    }
   ],
   "source": [
    "# set seed for reproducibility\n",
    "#print obs count\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------+-----------+-----+----------+\n",
      "|                          diff|label_string|probability|label|prediction|\n",
      "+------------------------------+------------+-----------+-----+----------+\n",
      "|--- \n",
      "\n",
      "+++ \n",
      "\n",
      "-|align=left st...|        safe|  [1.0,0.0]|  0.0|       0.0|\n",
      "+------------------------------+------------+-----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#do TF-IDF embeding, fit a pipeline, fit a logisti\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2) #minDocFreq: remove sparse terms\n",
    "#define the pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
    "#fit a pipeline and transform\n",
    "pipelineFit = pipeline.fit(df_wd)\n",
    "dataset = pipelineFit.transform(df_wd)\n",
    "#split dataset into train and test\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "#fit logistic regression\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "#predict on test set\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"diff\",\"label_string\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate model accuracy\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save logit model\n",
    "from pyspark.ml import PipelineModel\n",
    "outpath = 'C:/Pr/AA_Big_Data/Assignment_3/spark/output'\n",
    "lrModel.write().overwrite().save(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.classification import LogisticRegressionModel\n",
    "#model_in = LogisticRegressionModel.load(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "327px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
