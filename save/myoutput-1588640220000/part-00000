{"title_page": "Moore\u2013Penrose inverse", "text_new": "In [[mathematics]], and in particular [[linear algebra]], the '''Moore\u2013Penrose inverse''' {{tmath| A^+ }} of a  [[matrix (mathematics)|matrix]] {{tmath| A }} is the most widely known [[generalization]] of the [[inverse matrix]].{{sfn|Ben-Israel|Greville|2003|p=7}}{{sfn|Campbell|Meyer, Jr.|1991|p=10}}{{sfn|Nakamura|1991|p=42}}{{sfn|Rao|Mitra|1971|p=50\u201351}} It was independently described by [[E. H. Moore]]<ref name=\"Moore1920\">{{cite journal | last=Moore | first=E. H. | authorlink=E. H. Moore | title=On the reciprocal of the general algebraic matrix | journal=[[Bulletin of the American Mathematical Society]] | volume=26 |issue=9| pages=394\u201395 | year=1920 | url =http://projecteuclid.org/euclid.bams/1183425340 | doi = 10.1090/S0002-9904-1920-03322-7 | doi-access=free }}</ref> in 1920, [[Arne Bjerhammar]]<ref name=\"Bjerhammar1951\">{{cite journal | last=Bjerhammar| first=Arne| authorlink=Arne Bjerhammar | title=Application of calculus of matrices to method of least squares; with special references to geodetic calculations| journal=Trans. Roy. Inst. Tech. Stockholm | year=1951 | volume = 49}}</ref> in 1951, and [[Roger Penrose]]<ref name=\"Penrose1955\">{{cite journal | last=Penrose | first=Roger | authorlink=Roger Penrose | title=A generalized inverse for matrices | journal=[[Proceedings of the Cambridge Philosophical Society]] | volume=51 | issue=3 | pages=406\u201313 | year=1955 | doi=10.1017/S0305004100030401| bibcode=1955PCPS...51..406P | doi-access=free }}</ref> in 1955. Earlier, [[Erik Ivar Fredholm]] had introduced the concept of a pseudoinverse of [[integral operator]]s in 1903. When referring to a matrix, the term pseudoinverse, without further specification, is often used to indicate the Moore\u2013Penrose inverse. The term [[generalized inverse]] is sometimes used as a synonym for pseudoinverse.\n\nA common use of the pseudoinverse is to compute a \"best fit\" ([[Ordinary least squares|least squares]]) solution to a [[system of linear equations]] that lacks a unique solution (see below under [[#Applications|\u00a7 Applications]]).\nAnother use is to find the minimum ([[Euclidean norm|Euclidean]]) norm solution to a system of linear equations with multiple solutions. The ''pseudoinverse'' facilitates the statement and proof of results in linear algebra.\n\nThe pseudoinverse is defined and unique for all matrices whose entries are [[Real number|real]] or [[Complex number|complex]] numbers. It can be computed using the [[singular value decomposition]].\n\n==Notation==\nIn the following discussion, the following conventions are adopted.\n\n* {{tmath| \\mathbb{K} }} will denote one of the [[field (mathematics)|fields]] of real or complex numbers, denoted {{tmath| \\mathbb{R} }}, {{tmath| \\mathbb{C} }}, respectively. The vector space of {{tmath| m \\times n }} matrices over {{tmath| \\mathbb{K} }} is denoted by {{tmath| \\mathbb{K}^{m\\times n} }}.\n* For {{tmath| A \\in \\mathbb{K}^{m\\times n} }}, {{tmath| A^\\mathrm{T} }} and {{tmath| A^* }} denote the transpose and Hermitian transpose (also called [[conjugate transpose]]) respectively. If <math>\\mathbb{K} = \\mathbb{R}</math>, then <math>A^* = A^\\mathrm{T}</math>.\n* For {{tmath| A \\in \\mathbb{K}^{m\\times n} }}, {{tmath| \\operatorname{ran}(A) }} denotes the [[column space]] (image) of {{tmath| A }} (the space spanned by the column vectors of {{tmath| A }}) and {{tmath| \\operatorname{ker}(A) }} denotes the [[Kernel (linear algebra)|kernel]] (null space) of {{tmath| A }}.\n* Finally, for any positive integer {{tmath| n }}, {{tmath| I_n \\in \\mathbb{K}^{n\\times n} }} denotes the {{tmath| n \\times n }} [[identity matrix]].\n\n==Definition==\nFor {{tmath| A \\in \\mathbb{K}^{m\\times n} }}, a pseudoinverse of {{tmath| A }} is defined as a matrix {{tmath| A^+ \\in \\mathbb{K}^{n\\times m} }} satisfying all of the following four criteria, known as the Moore\u2013Penrose conditions:<ref name=\"Penrose1955\"/><ref name=\"GvL1996\">{{cite book | last=Golub | first=Gene H. | authorlink=Gene H. Golub |author2=Charles F. Van Loan  | title=Matrix computations | edition=3rd | publisher=Johns Hopkins | location=Baltimore | year=1996 | isbn=978-0-8018-5414-9 | pages = 257\u2013258| author2-link=Charles F. Van Loan }}</ref>\n\n:{|\n|-style=\"vertical-align:top;\"\n|style=\"width:6.6em;\"| {{tmath| \\text{1.}\\quad A A^+ A }} \n|style=\"width:7.5em;\"| {{tmath|1= = \\; A }}\n| ({{tmath| AA^+ }} need not be the general identity matrix, but it maps all column vectors of {{tmath| A }} to themselves);\n|-style=\"vertical-align:top;\"\n| {{tmath| \\text{2.}\\quad A^+ A A^+ }} ||{{tmath|1= = \\;  A^+ }} || ({{tmath| A^+ }} acts like a [[weak inverse]]);\n|-style=\"vertical-align:top;\"\n| {{tmath| \\text{3.}\\quad (AA^+)^* }}  ||{{tmath|1= = \\;  AA^+ }} || ({{tmath| AA^+ }} is [[Hermitian matrix|Hermitian]]);\n|-style=\"vertical-align:top;\"\n| {{tmath| \\text{4.}\\quad (A^+ A)^* }} ||{{tmath|1= = \\;  A^+ A }} || ({{tmath| A^+A }} is also Hermitian).\n|}\n\n{{tmath| A^+ }} exists for any matrix {{tmath| A }}, but, when the latter has full [[rank (linear algebra)|rank]] (that is, the rank of {{tmath| A }} is {{tmath| \\min \\{ m,n \\} }}), then {{tmath| A^+ }} can be expressed as a simple algebraic formula.\n\nIn particular, when {{tmath| A }} has linearly independent columns (and thus matrix {{tmath| A^* A }} is invertible), {{tmath| A^+ }} can be computed as\n: <math> A^+ = (A^* A)^{-1} A^*.</math>\n\nThis particular pseudoinverse constitutes a ''left inverse'', since, in this case, <math> A^+A = I </math>.\n\nWhen {{tmath| A }} has linearly independent rows (matrix {{tmath| A A^* }} is invertible), {{tmath| A^+ }} can be computed as\n: <math> A^+ = A^* (A A^*)^{-1}.</math>\n\nThis is a ''right inverse'', as <math> A A^+ = I</math>.\n\n==Properties==\n{{hatnote|Proofs for some of these facts may be found on a separate page, [[Proofs involving the Moore\u2013Penrose inverse]].}}\n\n===Existence and uniqueness===\nThe pseudoinverse exists and is unique: for any matrix {{tmath| A }}, there is precisely one matrix {{tmath| A^+ }}, that satisfies the four properties of the definition.<ref name=\"GvL1996\"/>\n\nA matrix satisfying the first condition of the definition is known as a [[generalized inverse]]. If the matrix also satisfies the second definition, it is called a [[generalized inverse#Types of generalized inverses|generalized ''reflexive'' inverse]]. Generalized inverses always exist but are not in general unique. Uniqueness is a consequence of the last two conditions.\n\n===Basic properties===\n* If {{tmath| A }} has real entries, then so does {{tmath| A^+ }}.\n* If {{tmath| A }} is [[invertible matrix|invertible]], its pseudoinverse is its inverse.  That is, <math>A^+ = A^{-1}</math>.<ref name=\"SB2002\">{{Cite book | last1=Stoer | first1=Josef | last2=Bulirsch | first2=Roland | title=Introduction to Numerical Analysis | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=3rd | isbn=978-0-387-95452-3 | year=2002}}.</ref>{{rp|243}}\n* The pseudoinverse of a [[zero matrix]] is its transpose.\n* The pseudoinverse of the pseudoinverse is the original matrix: <math>\\left(A^+\\right)^+ = A</math>.<ref name=\"SB2002\" />{{rp|245}}\n* Pseudoinversion commutes with transposition, conjugation, and taking the conjugate transpose:<ref name=\"SB2002\"/>{{rp|245}} <!-- reference only mentions the last bit -->\n*: <math>\\left(A^\\mathrm{T}\\right)^+ = \\left(A^+\\right)^\\mathrm{T}</math>, <math>\\left(\\overline{A}\\right)^+ = \\overline{A^+}</math>, <math>\\left(A^*\\right)^+ = \\left(A^+\\right)^*</math>.\n* The pseudoinverse of a scalar multiple of {{tmath| A }} is the reciprocal multiple of {{tmath| A^+ }}:\n*: <math>\\left(\\alpha A\\right)^+ = \\alpha^{-1} A^+</math> for {{tmath| \\alpha \\neq 0 }}.\n\n====Identities====\nThe following identities can be used to cancel certain subexpressions or expand expressions involving pseudoinverses. Proofs for these properties can be found in the [[Proofs involving the Moore\u2013Penrose inverse|proofs subpage]].\n: <math>\\begin{alignat}{3}\n  A^+ ={}& A^+    && A^{+*} && A^* \\\\\n      ={}& A^*    && A^{+*} && A^+, \\\\\n  A   ={}& A^{+*} && A^*    && A \\\\\n      ={}& A      && A^*    && A^{+*}, \\\\\n  A^* ={}& A^*    && A      && A^+ \\\\\n      ={}& A^+    && A      && A^*.\n\\end{alignat}</math>\n\n===Reduction to Hermitian case===\nThe computation of the pseudoinverse is reducible to its construction in the Hermitian case. This is possible through the equivalences:\n: <math>A^+ = \\left(A^*A\\right)^+ A^*,</math>\n: <math>A^+ = A^* \\left(AA^*\\right)^+,</math>\n\nas {{tmath| A^*A }} and {{tmath| AA^* }} are Hermitian.\n\n===Products===\nIf {{tmath| A \\in \\mathbb{K}^{m\\times n},\\ B \\in \\mathbb{K}^{n\\times p} }}, and if\n# {{tmath| A }} has orthonormal columns (that is, <math>A^*A = I_n</math>), &nbsp; or\n# {{tmath| B }} has orthonormal rows (that is, <math>BB^* = I_n</math>), &nbsp; or\n# {{tmath| A }} has all columns linearly independent (full column rank) and {{tmath| B }} has all rows linearly independent (full row rank), &nbsp; or\n# <math>B = A^*</math> (that is, {{tmath| B }} is the conjugate transpose of {{tmath| A }}),\nthen\n: {{tmath|1= (AB)^+ = B^+ A^+. }}\n\nThe last property yields the equalities\n: <math>\\begin{align}\n  \\left(A A^*\\right)^+ &= A^{+*} A^+, \\\\\n  \\left(A^* A\\right)^+ &= A^+ A^{+*}.\n\\end{align}</math>\n\nNB: The equality {{tmath|1= (AB)^+ = B^+ A^+ }} does not hold in general.\nSee the counterexample:\n\n: <math>\\Biggl( \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 1 & 1 \\end{pmatrix} \\Biggr)^+ = \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix}^+ = \\begin{pmatrix}\n \\tfrac12 & 0 \\\\ \\tfrac12 & 0 \\end{pmatrix} \\quad \\neq \\quad \\begin{pmatrix}\n \\tfrac14 & 0 \\\\ \\tfrac14 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & \\tfrac12 \\\\ 0 & \\tfrac12 \\end{pmatrix} \\begin{pmatrix} \\tfrac12 & 0 \\\\ \\tfrac12 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 1 & 1 \\end{pmatrix}^+ \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix}^+ </math>\n\n===Projectors===\n<math>P = AA^+</math> and <math>Q = A^+A</math> are [[projection (linear algebra)|orthogonal projection operators]], that is, they are  Hermitian (<math>P = P^*</math>, <math>Q = Q^*</math>) and idempotent (<math>P^2 = P</math> and <math>Q^2 = Q</math>). The following hold:\n* <math>PA = AQ = A</math> and <math>A^+ P = QA^+ = A^+</math>\n* {{tmath| P }} is the [[orthogonal projector]] onto the [[range of a function|range]] of {{tmath| A }} (which equals the [[orthogonal complement]] of the kernel of {{tmath| A^* }}).\n* {{tmath| Q }} is the orthogonal projector onto the range of {{tmath| A^* }} (which equals the orthogonal complement of the kernel of {{tmath| A }}).\n* <math>(I - Q) = \\left(I - A^+A\\right)</math> is the orthogonal projector onto the kernel of {{tmath| A }}.\n* <math>(I - P) = \\left(I - AA^+\\right)</math> is the orthogonal projector onto the kernel of {{tmath| A^* }}.<ref name=\"GvL1996\"/>\n\nThe last two properties imply the following identities:\n* <math>A\\,\\ \\left(I - A^+ A\\right)= \\left(I - AA^+\\right)A\\ \\ = 0</math>\n* <math>A^*\\left(I - AA^+\\right) = \\left(I - A^+A\\right)A^* = 0</math>\n\nAnother property is the following: if {{tmath| A \\in \\mathbb{K}^{n\\times n} }} is Hermitian and idempotent (true if and only if it represents an orthogonal projection), then, for any matrix {{tmath| B\\in \\mathbb{K}^{m\\times n} }} the following equation holds:<ref>{{cite journal|first=Anthony A.|last=Maciejewski|first2=Charles A.|last2=Klein|title=Obstacle Avoidance for Kinematically Redundant Manipulators in Dynamically Varying Environments|journal=International Journal of Robotics Research|volume=4|issue=3|pages=109\u2013117|year=1985|doi=10.1177/027836498500400308|hdl=10217/536|hdl-access=free}}</ref>\n: <math> A(BA)^+ = (BA)^+</math>\n\nThis can be proven by defining matrices <math>C = BA</math>, <math>D = A(BA)^+</math>, and checking that {{tmath| D }} is indeed a pseudoinverse for {{tmath| C }} by verifying that the defining properties of the pseudoinverse hold, when {{tmath| A }} is Hermitian and idempotent.\n\nFrom the last property it follows that, if {{tmath| A \\in \\mathbb{K}^{n\\times n} }} is Hermitian and idempotent, for any matrix {{tmath| B \\in \\mathbb{K}^{n\\times m} }}\n:<math>(AB)^+A = (AB)^+</math>\n\nFinally, if {{tmath| A }} is an orthogonal projection matrix, then its pseudoinverse trivially coincides with the matrix itself, that is, <math>A^+ = A</math>.\n\n===Geometric construction===\nIf we view the matrix as a linear map {{tmath| A:K^n \\to K^m }} over a field {{tmath| K }} then {{tmath| A^+: K^m \\to K^n }} can be decomposed as follows. We write {{tmath| \\oplus }} for the [[direct sum of modules|direct sum]], {{tmath| \\perp }} for the [[orthogonal complement]], {{tmath| \\operatorname{ker} }} for the [[kernel (linear algebra)|kernel]] of a map, and {{tmath| \\operatorname{ran} }} for the [[image (mathematics)|image]] of a map. Notice that <math>K^n = \\left(\\operatorname{ker} A\\right)^\\perp \\oplus \\operatorname{ker} A</math> and <math>K^m = \\operatorname{ran} A \\oplus \\left(\\operatorname{ran} A\\right)^\\perp</math>. The restriction <math> A: \\left(\\operatorname{ker} A\\right)^\\perp \\to \\operatorname{ran} A</math> is then an isomorphism. This implies that {{tmath| A^+ }} on {{tmath| \\operatorname{ran} A }} is the inverse of this isomorphism, and is zero on <math>\\left(\\operatorname{ran} A\\right)^\\perp . </math>\n\nIn other words: To find {{tmath| A^+b }} for given {{tmath| b }} in {{tmath| K^m }}, first project {{tmath| b }} orthogonally onto the range of {{tmath| A }}, finding a point {{tmath| p(b) }} in the range. Then form {{tmath| A^{-1}(\\{p(b)\\}) }}, that is, find those vectors in {{tmath| K^n }} that {{tmath| A }} sends to {{tmath| p(b) }}. This will be an affine subspace of {{tmath| K^n }} parallel to the kernel of {{tmath| A }}. The element of this subspace that has the smallest length (that is, is closest to the origin) is the answer {{tmath| A^+b }} we are looking for. It can be found by taking an arbitrary member of {{tmath| A^{-1}(\\{p(b)\\}) }} and projecting it orthogonally onto the orthogonal complement of the kernel of {{tmath| A }}.\n\nThis description is closely related to the [[Moore\u2013Penrose inverse#Minimum norm solution to a linear system|Minimum norm solution to a linear system]].\n\n===Subspaces===\n: <math>\\begin{align}\n  \\operatorname{ker}\\left(A^+\\right) &= \\operatorname{ker}\\left(A^*\\right) \\\\\n  \\operatorname{ran}\\left(A^+\\right) &= \\operatorname{ran}\\left(A^*\\right)\n\\end{align}</math>\n\n===Limit relations===\nThe pseudoinverse are limits:\n: <math>A^+ = \\lim_{\\delta \\searrow 0} \\left(A^* A + \\delta I\\right)^{-1} A^*\n            = \\lim_{\\delta \\searrow 0} A^* \\left(A A^* + \\delta I\\right)^{-1}\n</math>\n: (see [[Tikhonov regularization]]). These limits exist even if {{tmath| \\left(AA^*\\right)^{-1} }}  or {{tmath| \\left(A^*A\\right)^{-1} }} do not exist.<ref name=\"GvL1996\"/>{{rp|263}}\n\n===Continuity===\nIn contrast to ordinary matrix inversion, the process of taking pseudoinverses is not [[continuous function|continuous]]: if the sequence {{tmath| \\left(A_n\\right) }} converges to the matrix {{tmath| A }} (in the [[matrix norm|maximum norm or Frobenius norm]], say), then {{tmath| (A_n)^+ }} need not converge to {{tmath| A^+ }}. However, if all the matrices have the same rank, {{tmath| (A_n)^+ }} will converge to {{tmath| A^+ }}.<ref name=\"rakocevic1997\">{{cite journal | last=Rako\u010devi\u0107 | first=Vladimir | title=On continuity of the Moore\u2013Penrose and Drazin inverses | journal=Matemati\u010dki Vesnik | volume=49 | pages=163\u201372 | year=1997 | url =http://elib.mi.sanu.ac.rs/files/journals/mv/209/mv973404.pdf }}</ref>\n\n===Derivative===\nThe derivative of a real valued pseudoinverse matrix which has constant rank at a point {{tmath| x }} may be calculated in terms of the derivative of the original matrix:<ref>{{cite journal|title=The Differentiation of Pseudo-Inverses and Nonlinear Least Squares Problems Whose Variables Separate|first1=G. H.|last1=Golub|first2=V.|last2=Pereyra|journal=SIAM Journal on Numerical Analysis|volume=10|number=2|date=April 1973|pages=413\u201332|jstor=2156365|doi=10.1137/0710036|bibcode=1973SJNA...10..413G}}</ref>\n: <math>\n  \\frac{\\mathrm d}{\\mathrm d x} A^+(x) =\n    -A^+ \\left( \\frac{\\mathrm d}{\\mathrm d x} A \\right) A^+ ~+~\n     A^+ A^{+\\text{T}} \\left( \\frac{\\mathrm d}{\\mathrm d x} A^\\text{T} \\right) \\left(I - A A^+\\right) ~+~\n     \\left(I - A^+ A\\right) \\left( \\frac{\\text{d}}{\\text{d} x} A^\\text{T} \\right) A^{+\\text{T}} A^+\n</math>\n\n==Examples==\n\nSince for invertible matrices the pseudoinverse equals the usual inverse, only examples of non-invertible matrices are considered below.\n\n* For <math>\\mathbf{A} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix},\\,</math> the pseudoinverse is <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}.</math> (Generally, the pseudoinverse of a zero matrix is its transpose.) The uniqueness of this pseudoinverse can be seen from the requirement <math>A^+ = A^+ A A^+</math>, since multiplication by a zero matrix would always produce a zero matrix.\n\n* For <math>\\mathbf{A} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\end{pmatrix},\\,</math> the pseudoinverse is <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 0 \\end{pmatrix}.</math> <!--\n--> <br> Indeed, <!--\n--> <math>\\mathbf{A\\,A^+} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2}  \\end{pmatrix},\\,</math> and thus <!--\n--> <math>\\mathbf{A\\,A^+A} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 0\\end{pmatrix} = A.</math> <!--\n--> <br> Similarly, <!--\n--> <math>\\mathbf{A^+A} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix},\\,</math> <!--\n--> and thus <!--\n--> <math>\\mathbf{A^+A\\,A^+} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 0 \\end{pmatrix} = A^+.</math>\n\n* For <!--\n--> <math>\\mathbf{A} = \\begin{pmatrix} 1 & 0 \\\\ -1 & 0 \\end{pmatrix},\\ </math> <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{2} \\\\ 0 & 0 \\end{pmatrix}.</math>\n\n* For <!--\n--> <math>\\mathbf{A} = \\begin{pmatrix} 1 & 0 \\\\ 2 & 0 \\end{pmatrix},\\ </math> <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} \\frac{1}{5} & \\frac{2}{5} \\\\ 0 & 0 \\end{pmatrix}.</math> (The denominators are <math>5 = 1^2 + 2^2</math>.)\n\n* For<!--\n--> <math>\\mathbf{A} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix},\\ </math> <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} \\frac{1}{4} & \\frac{1}{4} \\\\ \\frac{1}{4} & \\frac{1}{4} \\end{pmatrix}.</math>\n\n* For <math>\\mathbf{A} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{pmatrix},\\,</math> the pseudoinverse is <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}.</math> <!--\n--> <br> Note that for this matrix, the [[inverse element#Matrices|left inverse]] exists and thus equals {{tmath| \\mathbf{A^+} }}, <!--\n--> indeed, <math>\\mathbf{A^+A} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}.</math>\n\n==Special cases==\n\n===Scalars===\nIt is also possible to define a pseudoinverse for scalars and vectors. This amounts to treating these as matrices. The pseudoinverse of a scalar {{tmath| x }} is zero if {{tmath| x }} is zero and the reciprocal of {{tmath| x }} otherwise:\n:<math>x^+ = \\begin{cases}\n  0,      & \\mbox{if }x = 0; \\\\ \n  x^{-1}, & \\mbox{otherwise}.\n\\end{cases}</math>\n\n===Vectors===\nThe pseudoinverse of the null (all zero) vector is the transposed null vector. The pseudoinverse of a non-null vector is the conjugate transposed vector divided by its squared magnitude:\n:<math>x^+ = \\begin{cases}\n  0^\\mathrm{T},      & \\mbox{if }x = 0; \\\\\n  {x^* \\over x^* x}, & \\mbox{otherwise}.\n\\end{cases}</math>\n\n===Linearly independent columns===\nIf the '''columns''' of {{tmath| A }} are [[linear independence|linearly independent]]\n(so that {{tmath| m \\ge n }}), then {{tmath| A^*A }} is invertible. In this case, an explicit formula is:{{sfn|Ben-Israel|Greville|2003}}\n:<math>A^+ = \\left(A^*A\\right)^{-1}A^*</math>.\n\nIt follows that {{tmath| A^+ }} is then a left inverse of {{tmath| A }}: &nbsp; <math>A^+ A = I_n</math>.\n\n===Linearly independent rows===\nIf the '''rows''' of {{tmath| A }} are linearly independent (so that {{tmath| m \\le n }}), then\n{{tmath| AA^* }} is invertible. In this case, an explicit formula is:\n:<math>A^+ = A^*\\left(AA^*\\right)^{-1}</math>.\n\nIt follows that {{tmath| A^+ }} is a right inverse of {{tmath| A }}: &nbsp; <math>A A^+ = I_m</math>.\n\n===Orthonormal columns or rows===\nThis is a special case of either full column rank or full row rank (treated above). If {{tmath| A }} has orthonormal columns (<math>A^*A = I_n</math>) or orthonormal rows (<math>AA^* = I_m</math>), then:\n:<math>A^+ = A^*</math>.\n\n===Orthogonal projection matrices===\nIf {{tmath| A }} is an orthogonal projection matrix, that is, <math>A = A^*</math> and <math>A^2 = A</math>, then the pseudoinverse trivially coincides with the matrix itself:\n:<math>A^+ = A</math>.\n\n===Circulant matrices===\nFor a [[circulant matrix]] {{tmath| C }}, the singular value decomposition is given by the [[Fourier transform]], that is, the singular values are the Fourier coefficients. Let {{tmath| \\mathcal{F} }} be the [[DFT matrix|Discrete Fourier Transform (DFT) matrix]], then<ref name=\"Stallings1972\">{{cite journal | last=Stallings | first=W. T. | authorlink=W. T. Stallings | title=The Pseudoinverse of an ''r''-Circulant Matrix | journal=[[Proceedings of the American Mathematical Society]] | volume=34 | issue=2 | pages=385\u201388 | year=1972 | doi=10.2307/2038377 | last2=Boullion | first2=T. L.| jstor=2038377 }}</ref>\n:<math>\\begin{align}\n    C &= \\mathcal{F}\\cdot\\Sigma\\cdot\\mathcal{F}^* \\\\\n  C^+ &= \\mathcal{F}\\cdot\\Sigma^+\\cdot\\mathcal{F}^*\n\\end{align}</math>\n\n==Construction==\n\n===Rank decomposition===\nLet {{tmath| r \\le \\min(m, n) }} denote the [[rank (matrix theory)|rank]] of {{tmath| A \\in K^{m\\times n} }}. Then {{tmath| A }} can be [[rank factorization|(rank) decomposed]] as\n<math>A = BC</math> where {{tmath| B \\in K^{m\\times r} }} and {{tmath| C \\in K^{r\\times n} }} are of rank {{tmath| r }}. Then <math>A^+ = C^+B^+ = C^*\\left(CC^*\\right)^{-1}\\left(B^*B\\right)^{-1}B^*</math>.\n\n===The QR method===\nFor <math>\\mathbb{K} \\in \\{ \\mathbb{R}, \\mathbb{C}\\}</math> computing the product {{tmath| AA^* }} or {{tmath| A^*A }} and their inverses explicitly is often a source of numerical rounding errors and computational cost in practice. An alternative approach using the [[QR decomposition]] of {{tmath| A }} may be used instead.\n\nConsider the case when {{tmath| A }} is of full column rank, so that <math>A^+ = (A^*A)^{-1}A^*</math>. Then the [[Cholesky decomposition]] <math>A^*A = R^*R</math>, where {{tmath| R }} is an [[upper triangular matrix]], may be used. Multiplication by the inverse is then done easily by solving a system with multiple right-hand sides,\n: <math>A^+ = (A^*A)^{-1}A^*  \\quad \\Leftrightarrow \\quad  (A^*A)A^+ = A^*  \\quad \\Leftrightarrow \\quad R^*RA^+ = A^* </math>\n\nwhich may be solved by [[forward substitution]] followed by [[back substitution]].\n\nThe Cholesky decomposition may be computed without forming {{tmath| A^*A }} explicitly, by alternatively using the [[QR decomposition]] of <math> A = QR</math>, where <math>Q</math> has orthonormal columns, <math> Q^*Q = I </math>, and {{tmath| R }} is upper triangular. Then\n: <math> A^*A \\,=\\, (QR)^*(QR) \\,=\\, R^*Q^*QR \\,=\\, R^*R</math>,\n\nso {{tmath| R }} is the Cholesky factor of {{tmath| A^*A }}.\n\nThe case of full row rank is treated similarly by using the formula <math>A^+ = A^*(AA^*)^{-1}</math> and using a similar argument, swapping the roles of {{tmath| A }} and {{tmath| A^* }}.\n\n===Singular value decomposition (SVD)===\nA computationally simple and accurate way to compute the pseudoinverse is by using the [[singular value decomposition]].{{sfn|Ben-Israel|Greville|2003}}<ref name=\"GvL1996\"/><ref name=\"SLEandPI\">[http://websites.uwlax.edu/twill/svd/systems/index.html Linear Systems & Pseudo-Inverse]</ref>  If <math>A = U\\Sigma V^*</math> is the singular value decomposition of {{tmath| A }}, then <math>A^+ = V\\Sigma^+ U^*</math>. For a [[rectangular diagonal matrix]] such as {{tmath| \\Sigma }}, we get the pseudoinverse by taking the reciprocal of each non-zero element on the diagonal, leaving the zeros in place, and then transposing the matrix. In numerical computation, only elements larger than some small tolerance are taken to be nonzero, and the others are replaced by zeros. For example, in the [[MATLAB]], [[GNU Octave]], or [[NumPy]] function <tt>pinv</tt>, the tolerance is taken to be {{math|''t'' {{=}} \u03b5\u22c5max(''m'', ''n'')\u22c5max(\u03a3)}}, where \u03b5 is the [[machine epsilon]].\n\nThe computational cost of this method is dominated by the cost of computing the SVD, which is several times higher than matrix\u2013matrix multiplication, even if a state-of-the art implementation (such as that of [[LAPACK]]) is used.\n\nThe above procedure shows why taking the pseudoinverse is not a continuous operation: if the original matrix {{tmath| A }} has a singular value 0 (a diagonal entry of the matrix {{tmath| \\Sigma }} above), then modifying {{tmath| A }} slightly may turn this zero into a tiny positive number, thereby affecting the pseudoinverse dramatically as we now have to take the reciprocal of a tiny number.\n\n===Block matrices===\n[[Block matrix pseudoinverse|Optimized approaches]] exist for calculating the pseudoinverse of block structured matrices.\n\n===The iterative method of Ben-Israel and Cohen===\nAnother method for computing the pseudoinverse (cf. [[Drazin inverse]]) uses the recursion\n:<math> A_{i+1} = 2A_i - A_i A A_i, </math>\n\nwhich is sometimes referred to as hyper-power sequence. This recursion produces a sequence converging quadratically to the pseudoinverse of {{tmath| A }} if it is started with an appropriate {{tmath| A_0 }} satisfying <math>A_0 A = \\left(A_0 A\\right)^*</math>. The choice <math>A_0 = \\alpha A^*</math> (where <math>0 < \\alpha < 2/\\sigma^2_1(A)</math>, with {{tmath| \\sigma_1(A) }} denoting the largest singular value of {{tmath| A }}) <ref>{{cite journal | last1=Ben-Israel | first1=Adi | last2=Cohen | first2=Dan | title=On Iterative Computation of Generalized Inverses and Associated Projections | journal=SIAM Journal on Numerical Analysis | volume=3 | issue=3 | pages=410\u201319 | year=1966 | jstor=2949637 | doi=10.1137/0703035 | bibcode=1966SJNA....3..410B }}[http://benisrael.net/COHEN-BI-ITER-GI.pdf pdf]</ref> has been argued not to be competitive to the method using the SVD mentioned above, because even for moderately ill-conditioned matrices it takes a long time before {{tmath| A_i }} enters the region of quadratic convergence.<ref>{{cite journal | last1=S\u00f6derstr\u00f6m | first1=Torsten | last2=Stewart | first2=G. W. | title=On the Numerical Properties of an Iterative Method for Computing the Moore\u2013Penrose Generalized Inverse | journal=SIAM Journal on Numerical Analysis | volume=11 | issue=1 | pages=61\u201374 | year=1974 | jstor=2156431 | doi=10.1137/0711008 | bibcode=1974SJNA...11...61S }}</ref> However, if started with {{tmath| A_0 }} already close to the Moore\u2013Penrose inverse and <math>A_0 A = \\left(A_0 A\\right)^*</math>, for example <math>A_0 := \\left(A^* A + \\delta I\\right)^{-1} A^*</math>, convergence is fast (quadratic).\n\n===Updating the pseudoinverse===\n\nFor the cases where {{tmath| A }} has full row or column rank, and the inverse of the correlation matrix ({{tmath| AA^* }} for {{tmath| A }} with full row rank or {{tmath| A^*A }} for full column rank) is already known, the pseudoinverse for matrices related to {{tmath| A }} can be computed by applying the [[Sherman\u2013Morrison\u2013Woodbury formula]] to update the inverse of the correlation matrix, which may need less work. In particular, if the related matrix differs from the original one by only a changed, added or deleted row or column, incremental algorithms exist that exploit the relationship.<ref name=\"G1992\">{{Cite thesis |first= Tino |last=Gram\u00df |title= Worterkennung mit einem k\u00fcnstlichen neuronalen Netzwerk |type=PhD dissertation |publisher= Georg-August-Universit\u00e4t zu G\u00f6ttingen |year = 1992 | oclc = 841706164 }}</ref><ref name=\"EMTIYAZ2008\">{{cite document |first=Mohammad |last=Emtiyaz |title=Updating Inverse of a Matrix When a Column is Added/Removed |date=February 27, 2008\n|url=https://emtiyaz.github.io/Writings/OneColInv.pdf }}</ref>\n\nSimilarly, it is possible to update the Cholesky factor when a row or column is added, without creating the inverse of the correlation matrix explicitly. However, updating the pseudoinverse in the general rank-deficient case is much more complicated.<ref>{{cite journal|last=Meyer, Jr.|first=Carl D.|title=Generalized inverses and ranks of block matrices|journal=SIAM J. Appl. Math.|volume=25|issue=4|date=1973|pages=597\u2013602|doi=10.1137/0125057}}</ref><ref>{{cite journal|last=Meyer, Jr.|first=Carl D.|title=Generalized inversion of modified matrices|journal=SIAM J. Appl. Math.|volume=24|issue=3|date=1973|pages=315\u201323|doi=10.1137/0124033}}</ref>\n\n===Software libraries===\nThe Python package [[NumPy]] provides a pseudoinverse calculation through its functions <code>matrix.I</code> and <code>linalg.pinv</code>; its <code>pinv</code> uses the SVD-based algorithm. [[SciPy]] adds a function <code>scipy.linalg.pinv</code> that uses a least-squares solver. High-quality implementations of SVD, QR, and back substitution are available in [[Singular value decomposition#Implementations|standard libraries]], such as [[LAPACK]].  Writing one's own implementation of SVD is a major programming project that requires a significant [[Floating point#Accuracy problems|numerical expertise]]. In special circumstances, such as [[parallel computing]] or [[embedded computing]], however, alternative implementations by QR or even the use of an explicit inverse might be preferable, and custom implementations may be unavoidable.\n\nThe MASS package for [[R (programming language)|R]] provides a calculation of the Moore\u2013Penrose inverse through the <code>ginv</code> function.<ref>{{cite web |url=https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/ginv.html |title=R: Generalized Inverse of a Matrix}}</ref> The <code>ginv</code> function calculates a pseudoinverse using the singular value decomposition provided by the <code>svd</code> function in the base R package. An alternative is to employ the <code>pinv</code> function available in the pracma package.\n\nThe [[GNU Octave|Octave programming language]] provides a pseudoinverse through the standard package function <code>pinv</code> and the <code>pseudo_inverse()</code> method.\n\nIn [[Julia (programming language)]], the LinearAlgebra package of the standard library provides an implementation of the Moore-Penrose pseudoinverse <code>pinv()</code> implemented via singular-value decomposition.<ref>{{cite web |url=https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.pinv |title=LinearAlgebra.pinv}}</ref>\n\n==Applications==\n\n===Linear least-squares===\n{{See also|Linear least squares (mathematics)}}\n\nThe pseudoinverse provides a [[linear least squares (mathematics)|least squares]] solution to a [[system of linear equations]].<ref name=\"Penrose1956\">{{cite journal | last=Penrose | first=Roger | title=On best approximate solution of linear matrix equations | journal=[[Proceedings of the Cambridge Philosophical Society]] | volume=52 | pages=17\u201319 | year=1956 | issue=1 | doi=10.1017/S0305004100030929| bibcode=1956PCPS...52...17P }}</ref>\nFor {{tmath| A \\in \\mathbb{K}^{m\\times n} }}, given a system of linear equations\n:<math>A x = b,</math>\n\nin general, a vector {{tmath| x }} that solves the system may not exist, or if one does exist, it may not be unique. The pseudoinverse solves the \"least-squares\" problem as follows:\n\n* {{tmath| \\forall x \\in \\mathbb{K}^n }}, we have <math>\\|Ax - b\\|_2 \\ge \\|Az - b\\|_2</math> where <math>z = A^+b</math> and <math>\\|\\cdot\\|_2</math> denotes the [[Euclidean norm]].  This weak inequality holds with equality if and only if <math>x = A^+b + \\left(I - A^+A\\right)w</math>  for any vector {{tmath| w }}; this provides an infinitude of minimizing solutions unless {{tmath| A }} has full column rank, in which case {{tmath| \\left(I - A^+A\\right) }}  is a zero matrix.<ref name=Planitz>{{cite journal|last=Planitz|first=M.|title=Inconsistent systems of linear equations|journal=Mathematical Gazette|volume=63|issue=425|date=October 1979|pages=181\u201385|doi=10.2307/3617890|jstor=3617890}}</ref> The solution with minimum Euclidean norm is {{tmath| z. }}<ref name=Planitz/>\n\nThis result is easily extended to systems with multiple right-hand sides, when the Euclidean norm is replaced by the Frobenius norm. Let {{tmath| B \\in \\mathbb{K}^{m\\times p} }}.\n\n* {{tmath| \\forall X \\in \\mathbb{K}^{n\\times p} }}, we have <math> \\|AX - B\\|_{\\mathrm{F}} \\ge \\|AZ -B\\|_{\\mathrm{F}}</math> where <math>Z = A^+B</math> and <math>\\|\\cdot\\|_{\\mathrm{F}} </math> denotes the [[Frobenius norm]].\n\n===Obtaining all solutions of a linear system===\n\nIf the linear system\n\n:<math>A x = b</math>\n\nhas any solutions, they are all given by<ref name=James>{{cite journal|last=James|first=M.|title=The generalised inverse|journal=Mathematical Gazette|volume=62|issue=420|date=June 1978|pages=109\u201314|doi=10.1017/S0025557200086460}}</ref>\n\n:<math>x = A^+ b + \\left[I - A^+ A\\right]w</math>\n\nfor arbitrary vector {{tmath| w }}. Solution(s) exist if and only if <math>AA^+ b = b</math>.<ref name=James/>  If the latter holds, then the solution is unique if and only if {{tmath| A }} has full column rank, in which case {{tmath| \\left[I - A^+ A\\right] }} is a zero matrix. If solutions exist but {{tmath| A }} does not have full column rank, then we have an [[indeterminate system]], all of whose infinitude of solutions are given by this last equation.\n\n===Minimum norm solution to a linear system===\nFor linear systems <math>Ax = b,</math> with non-unique solutions (such as under-determined systems), the pseudoinverse may be used to construct the solution of minimum [[Euclidean norm]]\n<math>\\|x\\|_2</math> among all solutions.\n\n* If <math>Ax = b</math> is satisfiable, the vector <math>z = A^+b</math> is a solution, and satisfies <math>\\|z\\|_2 \\le \\|x\\|_2</math> for all solutions.\n\nThis result is easily extended to systems with multiple right-hand sides, when the Euclidean norm is replaced by the Frobenius norm. Let {{tmath| B \\in \\mathbb{K}^{m\\times p} }}.\n\n* If <math>AX = B</math> is satisfiable, the matrix <math>Z = A^+B</math> is a solution, and satisfies <math>\\|Z\\|_{\\mathrm{F}} \\le \\|X\\|_{\\mathrm{F}}</math> for all solutions.\n\n===Condition number===\nUsing the pseudoinverse and a [[matrix norm]], one can define a [[condition number]] for any matrix:\n:<math>\\mbox{cond}(A) = \\|A\\| \\left\\|A^+\\right\\|.\\ </math>\n\nA large condition number implies that the problem of finding least-squares solutions to the corresponding system of linear equations is ill-conditioned in the sense that small errors in the entries of {{tmath| A }} can lead to huge errors in the entries of the solution.<ref name=hagen/>\n\n==Generalizations==\nBesides for matrices over real and complex numbers, the conditions hold for matrices over [[biquaternion]]s, also called \"complex quaternions\".<ref>{{cite arXiv |title=Matrix Theory over the Complex Quaternion Algebra | at=p.8, Theorem 3.5 |eprint=math/0004005 |ref=tian2000| last1=Tian | first1=Yongge | year=2000 }}</ref>\n\nIn order to solve more general least-squares problems, one can define Moore\u2013Penrose inverses for all continuous linear operators {{tmath| A: H_1 \\rarr H_2 }} between two [[Hilbert space]]s {{tmath| H_1 }} and {{tmath| H_2 }}, using the same four conditions as in our definition above. It turns out that not every continuous linear operator has a continuous linear pseudoinverse in this sense.<ref name=hagen>{{cite book|first1=Roland|last1=Hagen|first2=Steffen|last2=Roch|first3=Bernd|last3=Silbermann|title=C*-algebras and Numerical Analysis|publisher=CRC Press|year=2001|chapter=Section 2.1.2}}</ref> Those that do are precisely the ones whose range is [[closed set|closed]] in {{tmath| H_2 }}.\n\nIn [[abstract algebra]], a Moore\u2013Penrose inverse may be defined on a [[*-regular semigroup]]. This abstract definition coincides with the one in linear algebra.\n\n==See also==\n* [[Proofs involving the Moore\u2013Penrose inverse]]\n* [[Drazin inverse]]\n* [[Hat matrix]]\n* [[Inverse element]]\n* [[Linear least squares (mathematics)]]\n* [[Pseudo-determinant]]\n* [[Von Neumann regular ring]]\n\n==Notes==\n{{Reflist|30em}}\n\n==References==\n* {{cite book| first1 = Adi | last1 = Ben-Israel |author1-link=Adi Ben-Israel| first2 = Thomas N.E. | last2 = Greville |author2-link=Thomas N. E. Greville|title=Generalized inverses: Theory and applications|\nedition= 2nd |location= New York, NY | publisher =  Springer |year=2003| isbn = 978-0-387-00293-4 |ref=harv| doi = 10.1007/b97366 }}\n* {{cite book| first1 = S. L. | last1 = Campbell | first2 = C. D. | last2 = Meyer, Jr. | title= Generalized Inverses of Linear Transformations| url = https://archive.org/details/generalizedinver0000camp | url-access = registration |\npublisher=Dover |year=1991 | isbn = 978-0-486-66693-8 |ref=harv}}\n* {{cite book| first = Yoshihiko | last = Nakamura  | title= Advanced Robotics: Redundancy and Optimization|\npublisher=Addison-Wesley  |year= 1991 | isbn = 978-0201151985 |ref=harv}}\n* {{cite book| first1 = C. Radhakrishna | last1 = Rao | first2 = Sujit Kumar | last2 = Mitra| title = Generalized Inverse of Matrices and its Applications | url = https://archive.org/details/generalizedinver0000raoc | url-access = registration | publisher= John Wiley & Sons |location = New York |year= 1971 | page=[https://archive.org/details/generalizedinver0000raoc/page/240 240] | isbn = 978-0-471-70821-6 |ref=harv}}\n\n==External links==\n* [http://planetmath.org/Pseudoinverse Pseudoinverse on PlanetMath]\n* [http://people.revoledu.com/kardi/tutorial/LinearAlgebra/MatrixGeneralizedInverse.html Interactive program & tutorial of Moore\u2013Penrose Pseudoinverse]\n* {{planetmath reference|id=6067|title=Moore\u2013Penrose inverse}}\n* {{MathWorld|urlname=Pseudoinverse|title=Pseudoinverse}}\n* {{MathWorld|urlname=Moore-PenroseMatrixInverse|title=Moore\u2013Penrose Inverse}}\n* [https://arxiv.org/abs/1110.6882 The Moore\u2013Penrose Pseudoinverse. A Tutorial Review of the Theory]\n* [http://engineerjs.com/doc/ejs/engine/linalg-1/_pinv.html Online Moore-Penrose Inverse calculator]\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Moore-Penrose Pseudoinverse}}\n[[Category:Matrix theory]]\n[[Category:Singular value decomposition]]\n[[Category:Numerical linear algebra]]\n", "text_old": "In [[mathematics]], and in particular [[linear algebra]], the '''Moore\u2013Penrose inverse''' {{tmath| A^+ }} of a  [[matrix (mathematics)|matrix]] {{tmath| A }} is the most widely known [[generalization]] of the [[inverse matrix]].{{sfn|Ben-Israel|Greville|2003|p=7}}{{sfn|Campbell|Meyer, Jr.|1991|p=10}}{{sfn|Nakamura|1991|p=42}}{{sfn|Rao|Mitra|1971|p=50\u201351}} It was independently described by [[E. H. Moore]]<ref name=\"Moore1920\">{{cite journal | last=Moore | first=E. H. | authorlink=E. H. Moore | title=On the reciprocal of the general algebraic matrix | journal=[[Bulletin of the American Mathematical Society]] | volume=26 |issue=9| pages=394\u201395 | year=1920 | url =http://projecteuclid.org/euclid.bams/1183425340 | doi = 10.1090/S0002-9904-1920-03322-7 | doi-access=free }}</ref> in 1920, [[Arne Bjerhammar]]<ref name=\"Bjerhammar1951\">{{cite journal | last=Bjerhammar| first=Arne| authorlink=Arne Bjerhammar | title=Application of calculus of matrices to method of least squares; with special references to geodetic calculations| journal=Trans. Roy. Inst. Tech. Stockholm | year=1951 | volume = 49}}</ref> in 1951, and [[Roger Penrose]]<ref name=\"Penrose1955\">{{cite journal | last=Penrose | first=Roger | authorlink=Roger Penrose | title=A generalized inverse for matrices | journal=[[Proceedings of the Cambridge Philosophical Society]] | volume=51 | issue=3 | pages=406\u201313 | year=1955 | doi=10.1017/S0305004100030401| bibcode=1955PCPS...51..406P | doi-access=free }}</ref> in 1955. Earlier, [[Erik Ivar Fredholm]] had introduced the concept of a pseudoinverse of [[integral operator]]s in 1903. When referring to a matrix, the term pseudoinverse, without further specification, is often used to indicate the Moore\u2013Penrose inverse. The term [[generalized inverse]] is sometimes used as a synonym for pseudoinverse.\n\nA common use of the pseudoinverse is to compute a \"best fit\" ([[Ordinary least squares|least squares]]) solution to a [[system of linear equations]] that lacks a unique solution (see below under [[#Applications|\u00a7 Applications]]).\nAnother use is to find the minimum ([[Euclidean norm|Euclidean]]) norm solution to a system of linear equations with multiple solutions. The ''pseudoinverse'' facilitates the statement and proof of results in linear algebra.\n\nThe pseudoinverse is defined and unique for all matrices whose entries are [[Real number|real]] or [[Complex number|complex]] numbers. It can be computed using the [[singular value decomposition]].\n\n==Notation==\nIn the following discussion, the following conventions are adopted.\n\n* {{tmath| \\mathbb{K} }} will denote one of the [[field (mathematics)|fields]] of real or complex numbers, denoted {{tmath| \\mathbb{R} }}, {{tmath| \\mathbb{C} }}, respectively. The vector space of {{tmath| m \\times n }} matrices over {{tmath| \\mathbb{K} }} is denoted by {{tmath| \\mathbb{K}^{m\\times n} }}.\n* For {{tmath| A \\in \\mathbb{K}^{m\\times n} }}, {{tmath| A^\\mathrm{T} }} and {{tmath| A^* }} denote the transpose and Hermitian transpose (also called [[conjugate transpose]]) respectively. If <math>\\mathbb{K} = \\mathbb{R}</math>, then <math>A^* = A^\\mathrm{T}</math>.\n* For {{tmath| A \\in \\mathbb{K}^{m\\times n} }}, {{tmath| \\operatorname{ran}(A) }} denotes the [[column space]] (image) of {{tmath| A }} (the space spanned by the column vectors of {{tmath| A }}) and {{tmath| \\operatorname{ker}(A) }} denotes the [[Kernel (linear algebra)|kernel]] (null space) of {{tmath| A }}.\n* Finally, for any positive integer {{tmath| n }}, {{tmath| I_n \\in \\mathbb{K}^{n\\times n} }} denotes the {{tmath| n \\times n }} [[identity matrix]].\n\n==Definition==\nFor {{tmath| A \\in \\mathbb{K}^{m\\times n} }}, a pseudoinverse of {{tmath| A }} is defined as a matrix {{tmath| A^+ \\in \\mathbb{K}^{n\\times m} }} satisfying all of the following four criteria, known as the Moore\u2013Penrose conditions:<ref name=\"Penrose1955\"/><ref name=\"GvL1996\">{{cite book | last=Golub | first=Gene H. | authorlink=Gene H. Golub |author2=Charles F. Van Loan  | title=Matrix computations | edition=3rd | publisher=Johns Hopkins | location=Baltimore | year=1996 | isbn=978-0-8018-5414-9 | pages = 257\u2013258| author2-link=Charles F. Van Loan }}</ref>\n\n:{|\n|-style=\"vertical-align:top;\"\n|style=\"width:6.6em;\"| {{tmath| \\text{1.}\\quad A A^+ A }} \n|style=\"width:7.5em;\"| {{tmath|1= = \\; A }}\n| ({{tmath| AA^+ }} need not be the general identity matrix, but it maps all column vectors of {{tmath| A }} to themselves);\n|-style=\"vertical-align:top;\"\n| {{tmath| \\text{2.}\\quad A^+ A A^+ }} ||{{tmath|1= = \\;  A^+ }} || ({{tmath| A^+ }} acts like a [[weak inverse]]);\n|-style=\"vertical-align:top;\"\n| {{tmath| \\text{3.}\\quad (AA^+)^* }}  ||{{tmath|1= = \\;  AA^+ }} || ({{tmath| AA^+ }} is [[Hermitian matrix|Hermitian]]);\n|-style=\"vertical-align:top;\"\n| {{tmath| \\text{4.}\\quad (A^+ A)^* }} ||{{tmath|1= = \\;  A^+ A }} || ({{tmath| A^+A }} is also Hermitian).\n|}\n\n{{tmath| A^+ }} exists for any matrix {{tmath| A }}, but, when the latter has full [[rank (linear algebra)|rank]] (that is, the rank of {{tmath| A }} is {{tmath| \\min \\{ m,n \\} }}), then {{tmath| A^+ }} can be expressed as a simple algebraic formula.\n\nIn particular, when {{tmath| A }} has linearly independent rows (and thus matrix {{tmath| A^* A }} is invertible), {{tmath| A^+ }} can be computed as\n: <math> A^+ = (A^* A)^{-1} A^*.</math>\n\nThis particular pseudoinverse constitutes a ''left inverse'', since, in this case, <math> A^+A = I </math>.\n\nWhen {{tmath| A }} has linearly independent columns(matrix {{tmath| A A^* }} is invertible), {{tmath| A^+ }} can be computed as\n: <math> A^+ = A^* (A A^*)^{-1}.</math>\n\nThis is a ''right inverse'', as <math> A A^+ = I</math>.\n\n==Properties==\n{{hatnote|Proofs for some of these facts may be found on a separate page, [[Proofs involving the Moore\u2013Penrose inverse]].}}\n\n===Existence and uniqueness===\nThe pseudoinverse exists and is unique: for any matrix {{tmath| A }}, there is precisely one matrix {{tmath| A^+ }}, that satisfies the four properties of the definition.<ref name=\"GvL1996\"/>\n\nA matrix satisfying the first condition of the definition is known as a [[generalized inverse]]. If the matrix also satisfies the second definition, it is called a [[generalized inverse#Types of generalized inverses|generalized ''reflexive'' inverse]]. Generalized inverses always exist but are not in general unique. Uniqueness is a consequence of the last two conditions.\n\n===Basic properties===\n* If {{tmath| A }} has real entries, then so does {{tmath| A^+ }}.\n* If {{tmath| A }} is [[invertible matrix|invertible]], its pseudoinverse is its inverse.  That is, <math>A^+ = A^{-1}</math>.<ref name=\"SB2002\">{{Cite book | last1=Stoer | first1=Josef | last2=Bulirsch | first2=Roland | title=Introduction to Numerical Analysis | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=3rd | isbn=978-0-387-95452-3 | year=2002}}.</ref>{{rp|243}}\n* The pseudoinverse of a [[zero matrix]] is its transpose.\n* The pseudoinverse of the pseudoinverse is the original matrix: <math>\\left(A^+\\right)^+ = A</math>.<ref name=\"SB2002\" />{{rp|245}}\n* Pseudoinversion commutes with transposition, conjugation, and taking the conjugate transpose:<ref name=\"SB2002\"/>{{rp|245}} <!-- reference only mentions the last bit -->\n*: <math>\\left(A^\\mathrm{T}\\right)^+ = \\left(A^+\\right)^\\mathrm{T}</math>, <math>\\left(\\overline{A}\\right)^+ = \\overline{A^+}</math>, <math>\\left(A^*\\right)^+ = \\left(A^+\\right)^*</math>.\n* The pseudoinverse of a scalar multiple of {{tmath| A }} is the reciprocal multiple of {{tmath| A^+ }}:\n*: <math>\\left(\\alpha A\\right)^+ = \\alpha^{-1} A^+</math> for {{tmath| \\alpha \\neq 0 }}.\n\n====Identities====\nThe following identities can be used to cancel certain subexpressions or expand expressions involving pseudoinverses. Proofs for these properties can be found in the [[Proofs involving the Moore\u2013Penrose inverse|proofs subpage]].\n: <math>\\begin{alignat}{3}\n  A^+ ={}& A^+    && A^{+*} && A^* \\\\\n      ={}& A^*    && A^{+*} && A^+, \\\\\n  A   ={}& A^{+*} && A^*    && A \\\\\n      ={}& A      && A^*    && A^{+*}, \\\\\n  A^* ={}& A^*    && A      && A^+ \\\\\n      ={}& A^+    && A      && A^*.\n\\end{alignat}</math>\n\n===Reduction to Hermitian case===\nThe computation of the pseudoinverse is reducible to its construction in the Hermitian case. This is possible through the equivalences:\n: <math>A^+ = \\left(A^*A\\right)^+ A^*,</math>\n: <math>A^+ = A^* \\left(AA^*\\right)^+,</math>\n\nas {{tmath| A^*A }} and {{tmath| AA^* }} are Hermitian.\n\n===Products===\nIf {{tmath| A \\in \\mathbb{K}^{m\\times n},\\ B \\in \\mathbb{K}^{n\\times p} }}, and if\n# {{tmath| A }} has orthonormal columns (that is, <math>A^*A = I_n</math>), &nbsp; or\n# {{tmath| B }} has orthonormal rows (that is, <math>BB^* = I_n</math>), &nbsp; or\n# {{tmath| A }} has all columns linearly independent (full column rank) and {{tmath| B }} has all rows linearly independent (full row rank), &nbsp; or\n# <math>B = A^*</math> (that is, {{tmath| B }} is the conjugate transpose of {{tmath| A }}),\nthen\n: {{tmath|1= (AB)^+ = B^+ A^+. }}\n\nThe last property yields the equalities\n: <math>\\begin{align}\n  \\left(A A^*\\right)^+ &= A^{+*} A^+, \\\\\n  \\left(A^* A\\right)^+ &= A^+ A^{+*}.\n\\end{align}</math>\n\nNB: The equality {{tmath|1= (AB)^+ = B^+ A^+ }} does not hold in general.\nSee the counterexample:\n\n: <math>\\Biggl( \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 1 & 1 \\end{pmatrix} \\Biggr)^+ = \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix}^+ = \\begin{pmatrix}\n \\tfrac12 & 0 \\\\ \\tfrac12 & 0 \\end{pmatrix} \\quad \\neq \\quad \\begin{pmatrix}\n \\tfrac14 & 0 \\\\ \\tfrac14 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & \\tfrac12 \\\\ 0 & \\tfrac12 \\end{pmatrix} \\begin{pmatrix} \\tfrac12 & 0 \\\\ \\tfrac12 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 1 & 1 \\end{pmatrix}^+ \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix}^+ </math>\n\n===Projectors===\n<math>P = AA^+</math> and <math>Q = A^+A</math> are [[projection (linear algebra)|orthogonal projection operators]], that is, they are  Hermitian (<math>P = P^*</math>, <math>Q = Q^*</math>) and idempotent (<math>P^2 = P</math> and <math>Q^2 = Q</math>). The following hold:\n* <math>PA = AQ = A</math> and <math>A^+ P = QA^+ = A^+</math>\n* {{tmath| P }} is the [[orthogonal projector]] onto the [[range of a function|range]] of {{tmath| A }} (which equals the [[orthogonal complement]] of the kernel of {{tmath| A^* }}).\n* {{tmath| Q }} is the orthogonal projector onto the range of {{tmath| A^* }} (which equals the orthogonal complement of the kernel of {{tmath| A }}).\n* <math>(I - Q) = \\left(I - A^+A\\right)</math> is the orthogonal projector onto the kernel of {{tmath| A }}.\n* <math>(I - P) = \\left(I - AA^+\\right)</math> is the orthogonal projector onto the kernel of {{tmath| A^* }}.<ref name=\"GvL1996\"/>\n\nThe last two properties imply the following identities:\n* <math>A\\,\\ \\left(I - A^+ A\\right)= \\left(I - AA^+\\right)A\\ \\ = 0</math>\n* <math>A^*\\left(I - AA^+\\right) = \\left(I - A^+A\\right)A^* = 0</math>\n\nAnother property is the following: if {{tmath| A \\in \\mathbb{K}^{n\\times n} }} is Hermitian and idempotent (true if and only if it represents an orthogonal projection), then, for any matrix {{tmath| B\\in \\mathbb{K}^{m\\times n} }} the following equation holds:<ref>{{cite journal|first=Anthony A.|last=Maciejewski|first2=Charles A.|last2=Klein|title=Obstacle Avoidance for Kinematically Redundant Manipulators in Dynamically Varying Environments|journal=International Journal of Robotics Research|volume=4|issue=3|pages=109\u2013117|year=1985|doi=10.1177/027836498500400308|hdl=10217/536|hdl-access=free}}</ref>\n: <math> A(BA)^+ = (BA)^+</math>\n\nThis can be proven by defining matrices <math>C = BA</math>, <math>D = A(BA)^+</math>, and checking that {{tmath| D }} is indeed a pseudoinverse for {{tmath| C }} by verifying that the defining properties of the pseudoinverse hold, when {{tmath| A }} is Hermitian and idempotent.\n\nFrom the last property it follows that, if {{tmath| A \\in \\mathbb{K}^{n\\times n} }} is Hermitian and idempotent, for any matrix {{tmath| B \\in \\mathbb{K}^{n\\times m} }}\n:<math>(AB)^+A = (AB)^+</math>\n\nFinally, if {{tmath| A }} is an orthogonal projection matrix, then its pseudoinverse trivially coincides with the matrix itself, that is, <math>A^+ = A</math>.\n\n===Geometric construction===\nIf we view the matrix as a linear map {{tmath| A:K^n \\to K^m }} over a field {{tmath| K }} then {{tmath| A^+: K^m \\to K^n }} can be decomposed as follows. We write {{tmath| \\oplus }} for the [[direct sum of modules|direct sum]], {{tmath| \\perp }} for the [[orthogonal complement]], {{tmath| \\operatorname{ker} }} for the [[kernel (linear algebra)|kernel]] of a map, and {{tmath| \\operatorname{ran} }} for the [[image (mathematics)|image]] of a map. Notice that <math>K^n = \\left(\\operatorname{ker} A\\right)^\\perp \\oplus \\operatorname{ker} A</math> and <math>K^m = \\operatorname{ran} A \\oplus \\left(\\operatorname{ran} A\\right)^\\perp</math>. The restriction <math> A: \\left(\\operatorname{ker} A\\right)^\\perp \\to \\operatorname{ran} A</math> is then an isomorphism. This implies that {{tmath| A^+ }} on {{tmath| \\operatorname{ran} A }} is the inverse of this isomorphism, and is zero on <math>\\left(\\operatorname{ran} A\\right)^\\perp . </math>\n\nIn other words: To find {{tmath| A^+b }} for given {{tmath| b }} in {{tmath| K^m }}, first project {{tmath| b }} orthogonally onto the range of {{tmath| A }}, finding a point {{tmath| p(b) }} in the range. Then form {{tmath| A^{-1}(\\{p(b)\\}) }}, that is, find those vectors in {{tmath| K^n }} that {{tmath| A }} sends to {{tmath| p(b) }}. This will be an affine subspace of {{tmath| K^n }} parallel to the kernel of {{tmath| A }}. The element of this subspace that has the smallest length (that is, is closest to the origin) is the answer {{tmath| A^+b }} we are looking for. It can be found by taking an arbitrary member of {{tmath| A^{-1}(\\{p(b)\\}) }} and projecting it orthogonally onto the orthogonal complement of the kernel of {{tmath| A }}.\n\nThis description is closely related to the [[Moore\u2013Penrose inverse#Minimum norm solution to a linear system|Minimum norm solution to a linear system]].\n\n===Subspaces===\n: <math>\\begin{align}\n  \\operatorname{ker}\\left(A^+\\right) &= \\operatorname{ker}\\left(A^*\\right) \\\\\n  \\operatorname{ran}\\left(A^+\\right) &= \\operatorname{ran}\\left(A^*\\right)\n\\end{align}</math>\n\n===Limit relations===\nThe pseudoinverse are limits:\n: <math>A^+ = \\lim_{\\delta \\searrow 0} \\left(A^* A + \\delta I\\right)^{-1} A^*\n            = \\lim_{\\delta \\searrow 0} A^* \\left(A A^* + \\delta I\\right)^{-1}\n</math>\n: (see [[Tikhonov regularization]]). These limits exist even if {{tmath| \\left(AA^*\\right)^{-1} }}  or {{tmath| \\left(A^*A\\right)^{-1} }} do not exist.<ref name=\"GvL1996\"/>{{rp|263}}\n\n===Continuity===\nIn contrast to ordinary matrix inversion, the process of taking pseudoinverses is not [[continuous function|continuous]]: if the sequence {{tmath| \\left(A_n\\right) }} converges to the matrix {{tmath| A }} (in the [[matrix norm|maximum norm or Frobenius norm]], say), then {{tmath| (A_n)^+ }} need not converge to {{tmath| A^+ }}. However, if all the matrices have the same rank, {{tmath| (A_n)^+ }} will converge to {{tmath| A^+ }}.<ref name=\"rakocevic1997\">{{cite journal | last=Rako\u010devi\u0107 | first=Vladimir | title=On continuity of the Moore\u2013Penrose and Drazin inverses | journal=Matemati\u010dki Vesnik | volume=49 | pages=163\u201372 | year=1997 | url =http://elib.mi.sanu.ac.rs/files/journals/mv/209/mv973404.pdf }}</ref>\n\n===Derivative===\nThe derivative of a real valued pseudoinverse matrix which has constant rank at a point {{tmath| x }} may be calculated in terms of the derivative of the original matrix:<ref>{{cite journal|title=The Differentiation of Pseudo-Inverses and Nonlinear Least Squares Problems Whose Variables Separate|first1=G. H.|last1=Golub|first2=V.|last2=Pereyra|journal=SIAM Journal on Numerical Analysis|volume=10|number=2|date=April 1973|pages=413\u201332|jstor=2156365|doi=10.1137/0710036|bibcode=1973SJNA...10..413G}}</ref>\n: <math>\n  \\frac{\\mathrm d}{\\mathrm d x} A^+(x) =\n    -A^+ \\left( \\frac{\\mathrm d}{\\mathrm d x} A \\right) A^+ ~+~\n     A^+ A^{+\\text{T}} \\left( \\frac{\\mathrm d}{\\mathrm d x} A^\\text{T} \\right) \\left(I - A A^+\\right) ~+~\n     \\left(I - A^+ A\\right) \\left( \\frac{\\text{d}}{\\text{d} x} A^\\text{T} \\right) A^{+\\text{T}} A^+\n</math>\n\n==Examples==\n\nSince for invertible matrices the pseudoinverse equals the usual inverse, only examples of non-invertible matrices are considered below.\n\n* For <math>\\mathbf{A} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix},\\,</math> the pseudoinverse is <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}.</math> (Generally, the pseudoinverse of a zero matrix is its transpose.) The uniqueness of this pseudoinverse can be seen from the requirement <math>A^+ = A^+ A A^+</math>, since multiplication by a zero matrix would always produce a zero matrix.\n\n* For <math>\\mathbf{A} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\end{pmatrix},\\,</math> the pseudoinverse is <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 0 \\end{pmatrix}.</math> <!--\n--> <br> Indeed, <!--\n--> <math>\\mathbf{A\\,A^+} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{2}  \\end{pmatrix},\\,</math> and thus <!--\n--> <math>\\mathbf{A\\,A^+A} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 0\\end{pmatrix} = A.</math> <!--\n--> <br> Similarly, <!--\n--> <math>\\mathbf{A^+A} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix},\\,</math> <!--\n--> and thus <!--\n--> <math>\\mathbf{A^+A\\,A^+} = \\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 0 \\end{pmatrix} = A^+.</math>\n\n* For <!--\n--> <math>\\mathbf{A} = \\begin{pmatrix} 1 & 0 \\\\ -1 & 0 \\end{pmatrix},\\ </math> <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{2} \\\\ 0 & 0 \\end{pmatrix}.</math>\n\n* For <!--\n--> <math>\\mathbf{A} = \\begin{pmatrix} 1 & 0 \\\\ 2 & 0 \\end{pmatrix},\\ </math> <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} \\frac{1}{5} & \\frac{2}{5} \\\\ 0 & 0 \\end{pmatrix}.</math> (The denominators are <math>5 = 1^2 + 2^2</math>.)\n\n* For<!--\n--> <math>\\mathbf{A} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix},\\ </math> <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} \\frac{1}{4} & \\frac{1}{4} \\\\ \\frac{1}{4} & \\frac{1}{4} \\end{pmatrix}.</math>\n\n* For <math>\\mathbf{A} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{pmatrix},\\,</math> the pseudoinverse is <!--\n--> <math>\\mathbf{A^+} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}.</math> <!--\n--> <br> Note that for this matrix, the [[inverse element#Matrices|left inverse]] exists and thus equals {{tmath| \\mathbf{A^+} }}, <!--\n--> indeed, <math>\\mathbf{A^+A} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}.</math>\n\n==Special cases==\n\n===Scalars===\nIt is also possible to define a pseudoinverse for scalars and vectors. This amounts to treating these as matrices. The pseudoinverse of a scalar {{tmath| x }} is zero if {{tmath| x }} is zero and the reciprocal of {{tmath| x }} otherwise:\n:<math>x^+ = \\begin{cases}\n  0,      & \\mbox{if }x = 0; \\\\ \n  x^{-1}, & \\mbox{otherwise}.\n\\end{cases}</math>\n\n===Vectors===\nThe pseudoinverse of the null (all zero) vector is the transposed null vector. The pseudoinverse of a non-null vector is the conjugate transposed vector divided by its squared magnitude:\n:<math>x^+ = \\begin{cases}\n  0^\\mathrm{T},      & \\mbox{if }x = 0; \\\\\n  {x^* \\over x^* x}, & \\mbox{otherwise}.\n\\end{cases}</math>\n\n===Linearly independent columns===\nIf the '''columns''' of {{tmath| A }} are [[linear independence|linearly independent]]\n(so that {{tmath| m \\ge n }}), then {{tmath| A^*A }} is invertible. In this case, an explicit formula is:{{sfn|Ben-Israel|Greville|2003}}\n:<math>A^+ = \\left(A^*A\\right)^{-1}A^*</math>.\n\nIt follows that {{tmath| A^+ }} is then a left inverse of {{tmath| A }}: &nbsp; <math>A^+ A = I_n</math>.\n\n===Linearly independent rows===\nIf the '''rows''' of {{tmath| A }} are linearly independent (so that {{tmath| m \\le n }}), then\n{{tmath| AA^* }} is invertible. In this case, an explicit formula is:\n:<math>A^+ = A^*\\left(AA^*\\right)^{-1}</math>.\n\nIt follows that {{tmath| A^+ }} is a right inverse of {{tmath| A }}: &nbsp; <math>A A^+ = I_m</math>.\n\n===Orthonormal columns or rows===\nThis is a special case of either full column rank or full row rank (treated above). If {{tmath| A }} has orthonormal columns (<math>A^*A = I_n</math>) or orthonormal rows (<math>AA^* = I_m</math>), then:\n:<math>A^+ = A^*</math>.\n\n===Orthogonal projection matrices===\nIf {{tmath| A }} is an orthogonal projection matrix, that is, <math>A = A^*</math> and <math>A^2 = A</math>, then the pseudoinverse trivially coincides with the matrix itself:\n:<math>A^+ = A</math>.\n\n===Circulant matrices===\nFor a [[circulant matrix]] {{tmath| C }}, the singular value decomposition is given by the [[Fourier transform]], that is, the singular values are the Fourier coefficients. Let {{tmath| \\mathcal{F} }} be the [[DFT matrix|Discrete Fourier Transform (DFT) matrix]], then<ref name=\"Stallings1972\">{{cite journal | last=Stallings | first=W. T. | authorlink=W. T. Stallings | title=The Pseudoinverse of an ''r''-Circulant Matrix | journal=[[Proceedings of the American Mathematical Society]] | volume=34 | issue=2 | pages=385\u201388 | year=1972 | doi=10.2307/2038377 | last2=Boullion | first2=T. L.| jstor=2038377 }}</ref>\n:<math>\\begin{align}\n    C &= \\mathcal{F}\\cdot\\Sigma\\cdot\\mathcal{F}^* \\\\\n  C^+ &= \\mathcal{F}\\cdot\\Sigma^+\\cdot\\mathcal{F}^*\n\\end{align}</math>\n\n==Construction==\n\n===Rank decomposition===\nLet {{tmath| r \\le \\min(m, n) }} denote the [[rank (matrix theory)|rank]] of {{tmath| A \\in K^{m\\times n} }}. Then {{tmath| A }} can be [[rank factorization|(rank) decomposed]] as\n<math>A = BC</math> where {{tmath| B \\in K^{m\\times r} }} and {{tmath| C \\in K^{r\\times n} }} are of rank {{tmath| r }}. Then <math>A^+ = C^+B^+ = C^*\\left(CC^*\\right)^{-1}\\left(B^*B\\right)^{-1}B^*</math>.\n\n===The QR method===\nFor <math>\\mathbb{K} \\in \\{ \\mathbb{R}, \\mathbb{C}\\}</math> computing the product {{tmath| AA^* }} or {{tmath| A^*A }} and their inverses explicitly is often a source of numerical rounding errors and computational cost in practice. An alternative approach using the [[QR decomposition]] of {{tmath| A }} may be used instead.\n\nConsider the case when {{tmath| A }} is of full column rank, so that <math>A^+ = (A^*A)^{-1}A^*</math>. Then the [[Cholesky decomposition]] <math>A^*A = R^*R</math>, where {{tmath| R }} is an [[upper triangular matrix]], may be used. Multiplication by the inverse is then done easily by solving a system with multiple right-hand sides,\n: <math>A^+ = (A^*A)^{-1}A^*  \\quad \\Leftrightarrow \\quad  (A^*A)A^+ = A^*  \\quad \\Leftrightarrow \\quad R^*RA^+ = A^* </math>\n\nwhich may be solved by [[forward substitution]] followed by [[back substitution]].\n\nThe Cholesky decomposition may be computed without forming {{tmath| A^*A }} explicitly, by alternatively using the [[QR decomposition]] of <math> A = QR</math>, where <math>Q</math> has orthonormal columns, <math> Q^*Q = I </math>, and {{tmath| R }} is upper triangular. Then\n: <math> A^*A \\,=\\, (QR)^*(QR) \\,=\\, R^*Q^*QR \\,=\\, R^*R</math>,\n\nso {{tmath| R }} is the Cholesky factor of {{tmath| A^*A }}.\n\nThe case of full row rank is treated similarly by using the formula <math>A^+ = A^*(AA^*)^{-1}</math> and using a similar argument, swapping the roles of {{tmath| A }} and {{tmath| A^* }}.\n\n===Singular value decomposition (SVD)===\nA computationally simple and accurate way to compute the pseudoinverse is by using the [[singular value decomposition]].{{sfn|Ben-Israel|Greville|2003}}<ref name=\"GvL1996\"/><ref name=\"SLEandPI\">[http://websites.uwlax.edu/twill/svd/systems/index.html Linear Systems & Pseudo-Inverse]</ref>  If <math>A = U\\Sigma V^*</math> is the singular value decomposition of {{tmath| A }}, then <math>A^+ = V\\Sigma^+ U^*</math>. For a [[rectangular diagonal matrix]] such as {{tmath| \\Sigma }}, we get the pseudoinverse by taking the reciprocal of each non-zero element on the diagonal, leaving the zeros in place, and then transposing the matrix. In numerical computation, only elements larger than some small tolerance are taken to be nonzero, and the others are replaced by zeros. For example, in the [[MATLAB]], [[GNU Octave]], or [[NumPy]] function <tt>pinv</tt>, the tolerance is taken to be {{math|''t'' {{=}} \u03b5\u22c5max(''m'', ''n'')\u22c5max(\u03a3)}}, where \u03b5 is the [[machine epsilon]].\n\nThe computational cost of this method is dominated by the cost of computing the SVD, which is several times higher than matrix\u2013matrix multiplication, even if a state-of-the art implementation (such as that of [[LAPACK]]) is used.\n\nThe above procedure shows why taking the pseudoinverse is not a continuous operation: if the original matrix {{tmath| A }} has a singular value 0 (a diagonal entry of the matrix {{tmath| \\Sigma }} above), then modifying {{tmath| A }} slightly may turn this zero into a tiny positive number, thereby affecting the pseudoinverse dramatically as we now have to take the reciprocal of a tiny number.\n\n===Block matrices===\n[[Block matrix pseudoinverse|Optimized approaches]] exist for calculating the pseudoinverse of block structured matrices.\n\n===The iterative method of Ben-Israel and Cohen===\nAnother method for computing the pseudoinverse (cf. [[Drazin inverse]]) uses the recursion\n:<math> A_{i+1} = 2A_i - A_i A A_i, </math>\n\nwhich is sometimes referred to as hyper-power sequence. This recursion produces a sequence converging quadratically to the pseudoinverse of {{tmath| A }} if it is started with an appropriate {{tmath| A_0 }} satisfying <math>A_0 A = \\left(A_0 A\\right)^*</math>. The choice <math>A_0 = \\alpha A^*</math> (where <math>0 < \\alpha < 2/\\sigma^2_1(A)</math>, with {{tmath| \\sigma_1(A) }} denoting the largest singular value of {{tmath| A }}) <ref>{{cite journal | last1=Ben-Israel | first1=Adi | last2=Cohen | first2=Dan | title=On Iterative Computation of Generalized Inverses and Associated Projections | journal=SIAM Journal on Numerical Analysis | volume=3 | issue=3 | pages=410\u201319 | year=1966 | jstor=2949637 | doi=10.1137/0703035 | bibcode=1966SJNA....3..410B }}[http://benisrael.net/COHEN-BI-ITER-GI.pdf pdf]</ref> has been argued not to be competitive to the method using the SVD mentioned above, because even for moderately ill-conditioned matrices it takes a long time before {{tmath| A_i }} enters the region of quadratic convergence.<ref>{{cite journal | last1=S\u00f6derstr\u00f6m | first1=Torsten | last2=Stewart | first2=G. W. | title=On the Numerical Properties of an Iterative Method for Computing the Moore\u2013Penrose Generalized Inverse | journal=SIAM Journal on Numerical Analysis | volume=11 | issue=1 | pages=61\u201374 | year=1974 | jstor=2156431 | doi=10.1137/0711008 | bibcode=1974SJNA...11...61S }}</ref> However, if started with {{tmath| A_0 }} already close to the Moore\u2013Penrose inverse and <math>A_0 A = \\left(A_0 A\\right)^*</math>, for example <math>A_0 := \\left(A^* A + \\delta I\\right)^{-1} A^*</math>, convergence is fast (quadratic).\n\n===Updating the pseudoinverse===\n\nFor the cases where {{tmath| A }} has full row or column rank, and the inverse of the correlation matrix ({{tmath| AA^* }} for {{tmath| A }} with full row rank or {{tmath| A^*A }} for full column rank) is already known, the pseudoinverse for matrices related to {{tmath| A }} can be computed by applying the [[Sherman\u2013Morrison\u2013Woodbury formula]] to update the inverse of the correlation matrix, which may need less work. In particular, if the related matrix differs from the original one by only a changed, added or deleted row or column, incremental algorithms exist that exploit the relationship.<ref name=\"G1992\">{{Cite thesis |first= Tino |last=Gram\u00df |title= Worterkennung mit einem k\u00fcnstlichen neuronalen Netzwerk |type=PhD dissertation |publisher= Georg-August-Universit\u00e4t zu G\u00f6ttingen |year = 1992 | oclc = 841706164 }}</ref><ref name=\"EMTIYAZ2008\">{{cite document |first=Mohammad |last=Emtiyaz |title=Updating Inverse of a Matrix When a Column is Added/Removed |date=February 27, 2008\n|url=https://emtiyaz.github.io/Writings/OneColInv.pdf }}</ref>\n\nSimilarly, it is possible to update the Cholesky factor when a row or column is added, without creating the inverse of the correlation matrix explicitly. However, updating the pseudoinverse in the general rank-deficient case is much more complicated.<ref>{{cite journal|last=Meyer, Jr.|first=Carl D.|title=Generalized inverses and ranks of block matrices|journal=SIAM J. Appl. Math.|volume=25|issue=4|date=1973|pages=597\u2013602|doi=10.1137/0125057}}</ref><ref>{{cite journal|last=Meyer, Jr.|first=Carl D.|title=Generalized inversion of modified matrices|journal=SIAM J. Appl. Math.|volume=24|issue=3|date=1973|pages=315\u201323|doi=10.1137/0124033}}</ref>\n\n===Software libraries===\nThe Python package [[NumPy]] provides a pseudoinverse calculation through its functions <code>matrix.I</code> and <code>linalg.pinv</code>; its <code>pinv</code> uses the SVD-based algorithm. [[SciPy]] adds a function <code>scipy.linalg.pinv</code> that uses a least-squares solver. High-quality implementations of SVD, QR, and back substitution are available in [[Singular value decomposition#Implementations|standard libraries]], such as [[LAPACK]].  Writing one's own implementation of SVD is a major programming project that requires a significant [[Floating point#Accuracy problems|numerical expertise]]. In special circumstances, such as [[parallel computing]] or [[embedded computing]], however, alternative implementations by QR or even the use of an explicit inverse might be preferable, and custom implementations may be unavoidable.\n\nThe MASS package for [[R (programming language)|R]] provides a calculation of the Moore\u2013Penrose inverse through the <code>ginv</code> function.<ref>{{cite web |url=https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/ginv.html |title=R: Generalized Inverse of a Matrix}}</ref> The <code>ginv</code> function calculates a pseudoinverse using the singular value decomposition provided by the <code>svd</code> function in the base R package. An alternative is to employ the <code>pinv</code> function available in the pracma package.\n\nThe [[GNU Octave|Octave programming language]] provides a pseudoinverse through the standard package function <code>pinv</code> and the <code>pseudo_inverse()</code> method.\n\nIn [[Julia (programming language)]], the LinearAlgebra package of the standard library provides an implementation of the Moore-Penrose pseudoinverse <code>pinv()</code> implemented via singular-value decomposition.<ref>{{cite web |url=https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.pinv |title=LinearAlgebra.pinv}}</ref>\n\n==Applications==\n\n===Linear least-squares===\n{{See also|Linear least squares (mathematics)}}\n\nThe pseudoinverse provides a [[linear least squares (mathematics)|least squares]] solution to a [[system of linear equations]].<ref name=\"Penrose1956\">{{cite journal | last=Penrose | first=Roger | title=On best approximate solution of linear matrix equations | journal=[[Proceedings of the Cambridge Philosophical Society]] | volume=52 | pages=17\u201319 | year=1956 | issue=1 | doi=10.1017/S0305004100030929| bibcode=1956PCPS...52...17P }}</ref>\nFor {{tmath| A \\in \\mathbb{K}^{m\\times n} }}, given a system of linear equations\n:<math>A x = b,</math>\n\nin general, a vector {{tmath| x }} that solves the system may not exist, or if one does exist, it may not be unique. The pseudoinverse solves the \"least-squares\" problem as follows:\n\n* {{tmath| \\forall x \\in \\mathbb{K}^n }}, we have <math>\\|Ax - b\\|_2 \\ge \\|Az - b\\|_2</math> where <math>z = A^+b</math> and <math>\\|\\cdot\\|_2</math> denotes the [[Euclidean norm]].  This weak inequality holds with equality if and only if <math>x = A^+b + \\left(I - A^+A\\right)w</math>  for any vector {{tmath| w }}; this provides an infinitude of minimizing solutions unless {{tmath| A }} has full column rank, in which case {{tmath| \\left(I - A^+A\\right) }}  is a zero matrix.<ref name=Planitz>{{cite journal|last=Planitz|first=M.|title=Inconsistent systems of linear equations|journal=Mathematical Gazette|volume=63|issue=425|date=October 1979|pages=181\u201385|doi=10.2307/3617890|jstor=3617890}}</ref> The solution with minimum Euclidean norm is {{tmath| z. }}<ref name=Planitz/>\n\nThis result is easily extended to systems with multiple right-hand sides, when the Euclidean norm is replaced by the Frobenius norm. Let {{tmath| B \\in \\mathbb{K}^{m\\times p} }}.\n\n* {{tmath| \\forall X \\in \\mathbb{K}^{n\\times p} }}, we have <math> \\|AX - B\\|_{\\mathrm{F}} \\ge \\|AZ -B\\|_{\\mathrm{F}}</math> where <math>Z = A^+B</math> and <math>\\|\\cdot\\|_{\\mathrm{F}} </math> denotes the [[Frobenius norm]].\n\n===Obtaining all solutions of a linear system===\n\nIf the linear system\n\n:<math>A x = b</math>\n\nhas any solutions, they are all given by<ref name=James>{{cite journal|last=James|first=M.|title=The generalised inverse|journal=Mathematical Gazette|volume=62|issue=420|date=June 1978|pages=109\u201314|doi=10.1017/S0025557200086460}}</ref>\n\n:<math>x = A^+ b + \\left[I - A^+ A\\right]w</math>\n\nfor arbitrary vector {{tmath| w }}. Solution(s) exist if and only if <math>AA^+ b = b</math>.<ref name=James/>  If the latter holds, then the solution is unique if and only if {{tmath| A }} has full column rank, in which case {{tmath| \\left[I - A^+ A\\right] }} is a zero matrix. If solutions exist but {{tmath| A }} does not have full column rank, then we have an [[indeterminate system]], all of whose infinitude of solutions are given by this last equation.\n\n===Minimum norm solution to a linear system===\nFor linear systems <math>Ax = b,</math> with non-unique solutions (such as under-determined systems), the pseudoinverse may be used to construct the solution of minimum [[Euclidean norm]]\n<math>\\|x\\|_2</math> among all solutions.\n\n* If <math>Ax = b</math> is satisfiable, the vector <math>z = A^+b</math> is a solution, and satisfies <math>\\|z\\|_2 \\le \\|x\\|_2</math> for all solutions.\n\nThis result is easily extended to systems with multiple right-hand sides, when the Euclidean norm is replaced by the Frobenius norm. Let {{tmath| B \\in \\mathbb{K}^{m\\times p} }}.\n\n* If <math>AX = B</math> is satisfiable, the matrix <math>Z = A^+B</math> is a solution, and satisfies <math>\\|Z\\|_{\\mathrm{F}} \\le \\|X\\|_{\\mathrm{F}}</math> for all solutions.\n\n===Condition number===\nUsing the pseudoinverse and a [[matrix norm]], one can define a [[condition number]] for any matrix:\n:<math>\\mbox{cond}(A) = \\|A\\| \\left\\|A^+\\right\\|.\\ </math>\n\nA large condition number implies that the problem of finding least-squares solutions to the corresponding system of linear equations is ill-conditioned in the sense that small errors in the entries of {{tmath| A }} can lead to huge errors in the entries of the solution.<ref name=hagen/>\n\n==Generalizations==\nBesides for matrices over real and complex numbers, the conditions hold for matrices over [[biquaternion]]s, also called \"complex quaternions\".<ref>{{cite arXiv |title=Matrix Theory over the Complex Quaternion Algebra | at=p.8, Theorem 3.5 |eprint=math/0004005 |ref=tian2000| last1=Tian | first1=Yongge | year=2000 }}</ref>\n\nIn order to solve more general least-squares problems, one can define Moore\u2013Penrose inverses for all continuous linear operators {{tmath| A: H_1 \\rarr H_2 }} between two [[Hilbert space]]s {{tmath| H_1 }} and {{tmath| H_2 }}, using the same four conditions as in our definition above. It turns out that not every continuous linear operator has a continuous linear pseudoinverse in this sense.<ref name=hagen>{{cite book|first1=Roland|last1=Hagen|first2=Steffen|last2=Roch|first3=Bernd|last3=Silbermann|title=C*-algebras and Numerical Analysis|publisher=CRC Press|year=2001|chapter=Section 2.1.2}}</ref> Those that do are precisely the ones whose range is [[closed set|closed]] in {{tmath| H_2 }}.\n\nIn [[abstract algebra]], a Moore\u2013Penrose inverse may be defined on a [[*-regular semigroup]]. This abstract definition coincides with the one in linear algebra.\n\n==See also==\n* [[Proofs involving the Moore\u2013Penrose inverse]]\n* [[Drazin inverse]]\n* [[Hat matrix]]\n* [[Inverse element]]\n* [[Linear least squares (mathematics)]]\n* [[Pseudo-determinant]]\n* [[Von Neumann regular ring]]\n\n==Notes==\n{{Reflist|30em}}\n\n==References==\n* {{cite book| first1 = Adi | last1 = Ben-Israel |author1-link=Adi Ben-Israel| first2 = Thomas N.E. | last2 = Greville |author2-link=Thomas N. E. Greville|title=Generalized inverses: Theory and applications|\nedition= 2nd |location= New York, NY | publisher =  Springer |year=2003| isbn = 978-0-387-00293-4 |ref=harv| doi = 10.1007/b97366 }}\n* {{cite book| first1 = S. L. | last1 = Campbell | first2 = C. D. | last2 = Meyer, Jr. | title= Generalized Inverses of Linear Transformations| url = https://archive.org/details/generalizedinver0000camp | url-access = registration |\npublisher=Dover |year=1991 | isbn = 978-0-486-66693-8 |ref=harv}}\n* {{cite book| first = Yoshihiko | last = Nakamura  | title= Advanced Robotics: Redundancy and Optimization|\npublisher=Addison-Wesley  |year= 1991 | isbn = 978-0201151985 |ref=harv}}\n* {{cite book| first1 = C. Radhakrishna | last1 = Rao | first2 = Sujit Kumar | last2 = Mitra| title = Generalized Inverse of Matrices and its Applications | url = https://archive.org/details/generalizedinver0000raoc | url-access = registration | publisher= John Wiley & Sons |location = New York |year= 1971 | page=[https://archive.org/details/generalizedinver0000raoc/page/240 240] | isbn = 978-0-471-70821-6 |ref=harv}}\n\n==External links==\n* [http://planetmath.org/Pseudoinverse Pseudoinverse on PlanetMath]\n* [http://people.revoledu.com/kardi/tutorial/LinearAlgebra/MatrixGeneralizedInverse.html Interactive program & tutorial of Moore\u2013Penrose Pseudoinverse]\n* {{planetmath reference|id=6067|title=Moore\u2013Penrose inverse}}\n* {{MathWorld|urlname=Pseudoinverse|title=Pseudoinverse}}\n* {{MathWorld|urlname=Moore-PenroseMatrixInverse|title=Moore\u2013Penrose Inverse}}\n* [https://arxiv.org/abs/1110.6882 The Moore\u2013Penrose Pseudoinverse. A Tutorial Review of the Theory]\n* [http://engineerjs.com/doc/ejs/engine/linalg-1/_pinv.html Online Moore-Penrose Inverse calculator]\n\n{{Numerical linear algebra}}\n\n{{DEFAULTSORT:Moore-Penrose Pseudoinverse}}\n[[Category:Matrix theory]]\n[[Category:Singular value decomposition]]\n[[Category:Numerical linear algebra]]\n", "name_user": "Rdzhang", "label": "safe", "comment": "\u2192\u200eDefinition:Mistaken rows and columns.", "url_page": "//en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse"}
